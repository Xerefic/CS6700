{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Reinforcement Learning\n",
    "## Programming Assignment 1\n",
    "\n",
    "Submitted by:\n",
    "- Archish S (ME20B032)\n",
    "- Vinayak Gupta (EE20B152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_columns = 10\n",
    "start_state = {\n",
    "    \"s1\": np.array([[0, 4]]),\n",
    "    \"s2\": np.array([[3, 6]]),\n",
    "}\n",
    "obstructions = np.array([[0, 7], [1, 1], [1, 2], [1, 3], [1, 7], [2, 1], [2, 3],\n",
    "                        [2, 7], [3, 1], [3, 3], [3, 5], [4, 3], [4, 5], [4, 7],\n",
    "                        [5, 3], [5, 7], [5, 9], [6, 3], [6, 9], [7, 1], [7, 6],\n",
    "                        [7, 7], [7, 8], [7, 9], [8, 1], [8, 5], [8, 6], [9, 1]])\n",
    "\n",
    "bad_states = np.array([[1, 9], [4, 2], [4, 4], [7, 5], [9, 9]])\n",
    "restart_states = np.array([[3, 7], [8, 2]])\n",
    "goal_states = np.array([[0, 9], [2, 2], [8, 7]])\n",
    "\n",
    "step_reward = -1\n",
    "goal_reward = 10\n",
    "bad_state_reward = -6\n",
    "restart_state_reward = -100\n",
    "\n",
    "p_good_transition = 1\n",
    "bias = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnv):\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind=False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward=None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = GridWorld.row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = GridWorld.row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = GridWorld.row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states, self.num_states, self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1, 2, 1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col), 1)==0):\n",
    "                        next_state = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state, :, :] = 0\n",
    "                        self.P[state, next_state, :] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2, 3, 1, 0]\n",
    "        right = [3, 2, 0, 1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1, 1, 0, 0]\n",
    "        col_change = [0, 0, -1, 1]\n",
    "        row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0, 0] += row_change[direction]\n",
    "        row_col[0, 1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the gridworld with the start, goal, and bad states.\n",
    "        Mark X for Obstructions, G for Goal, B for Bad, and S for Start\n",
    "        \"\"\"\n",
    "        grid = np.zeros((self.num_rows, self.num_cols))\n",
    "        if self.obs_states is not None:\n",
    "            for i in range(self.obs_states.shape[0]):\n",
    "                grid[self.obs_states[i, 0], self.obs_states[i, 1]] = -1\n",
    "        for i in range(self.goal_states.shape[0]):\n",
    "            grid[self.goal_states[i, 0], self.goal_states[i, 1]] = 1\n",
    "        for i in range(self.bad_states.shape[0]):\n",
    "            grid[self.bad_states[i, 0], self.bad_states[i, 1]] = -2\n",
    "        for i in range(self.restart_states.shape[0]):\n",
    "            grid[self.restart_states[i, 0], self.restart_states[i, 1]] = -3\n",
    "        grid[self.start_state[0, 0], self.start_state[0, 1]] = 2\n",
    "        sns.heatmap(grid, annot=False, cmap=\"coolwarm\", cbar=False, linewidths=0.5, linecolor='black')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                if grid[i, j] == 1:\n",
    "                    plt.text(j+0.5, i+0.5, 'G', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == -2:\n",
    "                    plt.text(j+0.5, i+0.5, 'B', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == -3:\n",
    "                    plt.text(j+0.5, i+0.5, 'R', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 2:\n",
    "                    plt.text(j+0.5, i+0.5, 'S', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == -1:\n",
    "                    plt.text(j+0.5, i+0.5, 'X', ha='center', va='center', fontsize=10)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if (self.wind and np.random.random() < 0.4):\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]\n",
    "\n",
    "    @staticmethod\n",
    "    def row_col_to_seq(row_col, num_cols):\n",
    "        #Converts state number to row_column format\n",
    "        return row_col[:, 0] * num_cols + row_col[:, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def seq_to_col_row(seq, num_cols): \n",
    "        #Converts row_column format to state number\n",
    "        r = floor(seq / num_cols)\n",
    "        c = seq - r * num_cols\n",
    "        return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(state, wind=False, p_good_transition=1.0):\n",
    "    gw = GridWorld(\n",
    "        num_rows=num_rows,\n",
    "        num_cols=num_columns,\n",
    "        start_state=start_state[state],\n",
    "        goal_states=goal_states,\n",
    "        wind=wind\n",
    "    )\n",
    "    gw.add_obstructions(\n",
    "        obstructed_states=obstructions,\n",
    "        bad_states=bad_states,\n",
    "        restart_states=restart_states\n",
    "    )\n",
    "    gw.add_transition_probability(\n",
    "        p_good_transition=p_good_transition,\n",
    "        bias=bias\n",
    "    )\n",
    "    gw.add_rewards(\n",
    "        step_reward=step_reward,\n",
    "        goal_reward=goal_reward,\n",
    "        bad_state_reward=bad_state_reward,\n",
    "        restart_state_reward=restart_state_reward\n",
    "    )\n",
    "    env = gw.create_gridworld()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'greedy'\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return self.actions[np.argmax(action_values[state, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-Greedy Policy\n",
    "\n",
    "The $\\varepsilon$-greedy policy defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    \\underset{a \\in A(s)}{\\arg\\max} \\; Q(s, a) & \\text{with probability } 1 - \\varepsilon \\\\\n",
    "    \\textrm{random choice} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\varepsilon$: The probability of choosing a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpGreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'ep-greedy ep:{self.epsilon}'\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "\n",
    "        if np.random.binomial(1, 1-self.epsilon):\n",
    "            return self.actions[np.argmax(action_values[state, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Policy\n",
    "\n",
    "The softmax policy is defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    a_1 & \\text{with probability } \\mathcal{P}(1) \\\\\n",
    "    a_2 & \\text{with probability } \\mathcal{P}(2) \\\\\n",
    "    \\vdots  & \\vdots \\\\\n",
    "    a_n & \\text{with probability } \\mathcal{P}(n)\n",
    "\\end{cases}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathcal{P}(a) = \\dfrac{e^{Q(s, a) / \\tau}}{\\sum\\limits_{i=1}^{n} e^{Q(s, i) / \\tau}}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\tau$: The temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class SoftmaxPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'softmax tau:{self.tau}'\n",
    "\n",
    "    def __init__(self, tau, actions):\n",
    "        self.tau = tau\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return np.random.choice(self.actions, p = softmax(action_values[state, :]/self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "The update rule for SARSA:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'sarsa'\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * Q[next_state, next_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "The update rule for Q-Learning:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'q-learning'\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * np.max(Q[next_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyTrainer:\n",
    "    def __init__(self, env, exploration_policy, update_policy, episodes, runs):\n",
    "        self.env = env\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.update_policy = update_policy\n",
    "        self.episodes = episodes\n",
    "        self.runs = runs\n",
    "\n",
    "        self.steps = np.zeros((runs, episodes))\n",
    "        self.rewards = np.zeros((runs, episodes))\n",
    "        self.Q = np.zeros((runs, env.num_states, env.num_actions))\n",
    "        self.hmap_visits = np.zeros((runs, env.num_states))\n",
    "        self.hmap_Q = np.zeros((runs, env.num_states))\n",
    "\n",
    "    def train(self):\n",
    "        np.random.seed(32+152)\n",
    "\n",
    "        for run in tqdm.trange(self.runs, desc='Training Runs'):\n",
    "            for episode in range(self.episodes):\n",
    "\n",
    "                current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "                current_action = self.exploration_policy.select_action(current_state, self.Q[run])\n",
    "\n",
    "                self.steps[run, episode] = 0\n",
    "                self.rewards[run, episode] = 0\n",
    "                self.hmap_visits[run, current_state] += 1\n",
    "\n",
    "                while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and self.steps[run, episode] < 100:\n",
    "\n",
    "                    next_state, reward = self.env.step(current_state, current_action)\n",
    "                    next_action = self.exploration_policy.select_action(next_state, self.Q[run])\n",
    "\n",
    "                    self.Q[run, current_state, current_action] = self.update_policy.update(self.Q[run], current_state, current_action, next_state, next_action, reward)\n",
    "\n",
    "                    if current_state != next_state:\n",
    "                        self.hmap_visits[run, next_state] += 1\n",
    "\n",
    "                    current_state = next_state\n",
    "                    current_action = next_action\n",
    "\n",
    "                    self.steps[run, episode] += 1\n",
    "                    self.rewards[run, episode] += reward\n",
    "\n",
    "                if current_state not in list(GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols)):\n",
    "                    self.steps[run, episode] = np.inf\n",
    "\n",
    "            for state in range(self.env.num_states):\n",
    "                self.hmap_Q[run, state] = np.max(self.Q[run, state, :])\n",
    "\n",
    "    def plot_policy(self):\n",
    "        Q = np.mean(self.Q, axis=0)\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_policy.jpg'\n",
    "\n",
    "        policy = GreedyPolicy(np.arange(self.env.num_actions))\n",
    "        hmap_visits = np.zeros(self.env.num_states)\n",
    "        hmap_visits[GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)] = 1\n",
    "\n",
    "        current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "        current_action = policy.select_action(current_state, Q)\n",
    "\n",
    "        steps = 0\n",
    "        rewards = 0\n",
    "\n",
    "        \n",
    "        path = [current_state]\n",
    "        while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and steps < 100:\n",
    "                \n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = policy.select_action(next_state, Q)\n",
    "    \n",
    "                hmap_visits[next_state] = 1\n",
    "    \n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "    \n",
    "                steps += 1\n",
    "                rewards += reward\n",
    "                path.append(current_state)\n",
    "\n",
    "        plt.title(\"Learnt Policy\")\n",
    "        # hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        self.env.plot()\n",
    "        plt.plot([x % self.env.num_cols + 0.5 for x in path], [x // self.env.num_cols + 0.5 for x in path], 'r--')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_reward(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_reward.jpg'\n",
    "    \n",
    "        plt.title(f\"Reward per Episode: Avg:{round(np.mean(self.rewards), 3)}, Max:{round(np.max(self.rewards), 3)}\")\n",
    "\n",
    "        plt.plot(self.rewards.mean(axis=0), 'r')\n",
    "        plt.fill_between(range(self.episodes), self.rewards.mean(axis=0) - self.rewards.std(axis=0), self.rewards.mean(axis=0) + self.rewards.std(axis=0), alpha=0.2, color='r')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        \n",
    "    def plot_steps(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_steps.jpg'\n",
    "\n",
    "        plt.title(f\"Steps per Episode: Avg:{round(np.mean(self.steps), 3)}, Max:{round(np.max(self.steps), 3)}, Min:{round(np.min(self.steps), 3)}\")\n",
    "        \n",
    "        plt.plot(self.steps.mean(axis=0), 'b')\n",
    "        plt.fill_between(range(self.episodes), self.steps.mean(axis=0) - self.steps.std(axis=0), self.steps.mean(axis=0) + self.steps.std(axis=0), alpha=0.2, color='b')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_visits(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_heatmap.jpg'\n",
    "\n",
    "        plt.title(\"State Visits\")\n",
    "        hmap_visits = np.mean(self.hmap_visits, axis=0)\n",
    "        hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_Q(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_Q.jpg'\n",
    "\n",
    "        plt.title(f\"State Value Function: Avg:{round(np.mean(self.hmap_Q), 3)}, Max:{round(np.max(self.hmap_Q), 3)}, Min:{round(np.min(self.hmap_Q), 3)}\")\n",
    "        Q = np.mean(self.hmap_Q, axis=0)\n",
    "        hmap = sns.heatmap(Q.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct hyperparameter tuning, we opt for maximizing asymptotic optimality, which entails leveraging Q-values acquired by the agent and employing a greedy action selection method. \n",
    "\n",
    "Following this approach, we establish a grid search function to determine the optimal hyperparameter set based on asymptotic optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_grid_search(env, alphas, gammas, epsilons, taus, model = 'sarsa', policy = 'epsilon'):\n",
    "    optimal_reward = - np.inf\n",
    "    optimal_hyperparams = {}\n",
    "\n",
    "    if policy == \"softmax\":\n",
    "    # Softmax\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for tau in taus:\n",
    "                    print(f\"The current set of Hyperparams: alpha = {alpha}, gamma = {gamma}, tau = {tau}\")\n",
    "                    if model == \"sarsa\":\n",
    "                        update_policy = SARSAUpdate(alpha=alpha, gamma=gamma)\n",
    "                    elif model == \"qlearning\":\n",
    "                        update_policy = QLearningUpdate(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "                    exploration_policy = SoftmaxPolicy(tau=tau, actions=np.arange(env.num_actions))\n",
    "                    trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "                    trainer.train()\n",
    "                    reward = trainer.rewards\n",
    "                    mean_reward = np.mean(np.mean(reward, axis = 1), axis = 0)\n",
    "                    if optimal_reward < mean_reward:\n",
    "                        optimal_reward = mean_reward\n",
    "                        optimal_hyperparams = {}\n",
    "                        optimal_hyperparams[\"alpha\"] = alpha\n",
    "                        optimal_hyperparams[\"gamma\"] = gamma\n",
    "                        optimal_hyperparams[\"tau\"] = tau\n",
    "    \n",
    "    elif policy == \"epsilon\":\n",
    "    # Epsilon\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for epsilon in epsilons:\n",
    "                    print(f\"The current set of Hyperparams: alpha = {alpha}, gamma = {gamma}, epsilon = {epsilon}\")\n",
    "                    if model == \"sarsa\":\n",
    "                        update_policy = SARSAUpdate(alpha=alpha, gamma=gamma)\n",
    "                    elif model == \"qlearning\":\n",
    "                        update_policy = QLearningUpdate(alpha=alpha, gamma=gamma)\n",
    "                        \n",
    "                    exploration_policy = EpGreedyPolicy(epsilon=epsilon, actions=np.arange(env.num_actions))\n",
    "                    trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "                    trainer.train()\n",
    "                    reward = trainer.rewards\n",
    "                    mean_reward = np.mean(np.mean(reward, axis = 1), axis = 0)\n",
    "                    if optimal_reward < mean_reward:\n",
    "                        optimal_reward = mean_reward\n",
    "                        optimal_hyperparams = {}\n",
    "                        optimal_hyperparams[\"alpha\"] = alpha\n",
    "                        optimal_hyperparams[\"gamma\"] = gamma\n",
    "                        optimal_hyperparams[\"epsilon\"] = epsilons\n",
    "\n",
    "    return optimal_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$: 0.001, 0.01, 0.1, 0.2\n",
    "\n",
    "$\\gamma$: 0.7, 0.8, 0.9, 1\n",
    "\n",
    "$\\epsilon$:  0.001, 0.01, 0.1, 0.5\n",
    "\n",
    "$\\tau$: 0.01, 0.1, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 0.2]\n",
    "gammas = [0.7, 0.8, 0.9, 1]\n",
    "epsilons = [0.001, 0.01, 0.1, 0.5]\n",
    "tau = [0.01, 0.1, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "State = s1 (0, 4) \n",
    "Wind = False \n",
    "p = 1 \n",
    "SARSA Algo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.001, gamma = 0.7, tau = 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs:   0%|          | 0/5 [00:00<?, ?it/s]/var/folders/vn/q3t9y55d35v26w9jqv0sf6p40000gn/T/ipykernel_1246/317266452.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.Q[run, current_state, current_action] = self.update_policy.update(self.Q[run], current_state, current_action, next_state, next_action, reward)\n",
      "/var/folders/vn/q3t9y55d35v26w9jqv0sf6p40000gn/T/ipykernel_1246/317266452.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.rewards[run, episode] += reward\n",
      "Training Runs: 100%|██████████| 5/5 [02:10<00:00, 26.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.001, gamma = 0.7, tau = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [02:16<00:00, 27.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.001, gamma = 0.7, tau = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [02:21<00:00, 28.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.001, gamma = 0.7, tau = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [02:19<00:00, 27.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.01, gamma = 0.7, tau = 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [00:49<00:00,  9.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.01, gamma = 0.7, tau = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [01:30<00:00, 18.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.01, gamma = 0.7, tau = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [02:14<00:00, 26.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.01, gamma = 0.7, tau = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [02:16<00:00, 27.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.1, gamma = 0.7, tau = 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [00:32<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.1, gamma = 0.7, tau = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [01:18<00:00, 15.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.1, gamma = 0.7, tau = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [02:03<00:00, 24.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.1, gamma = 0.7, tau = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [08:56<00:00, 107.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.2, gamma = 0.7, tau = 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 5/5 [57:46<00:00, 693.30s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current set of Hyperparams: alpha = 0.2, gamma = 0.7, tau = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs:   0%|          | 0/5 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m get_env(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms1\u001b[39m\u001b[38;5;124m'\u001b[39m, wind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m optimal_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mreward_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgammas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msarsa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(optimal_hyperparams\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Softmax is Best\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     exploration_policy \u001b[38;5;241m=\u001b[39m SoftmaxPolicy(tau\u001b[38;5;241m=\u001b[39moptimal_hyperparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m\"\u001b[39m], actions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(env\u001b[38;5;241m.\u001b[39mnum_actions))\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mreward_grid_search\u001b[0;34m(env, alphas, gammas, epsilons, taus, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m exploration_policy \u001b[38;5;241m=\u001b[39m SoftmaxPolicy(tau\u001b[38;5;241m=\u001b[39mtau, actions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(env\u001b[38;5;241m.\u001b[39mnum_actions))\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PolicyTrainer(env, exploration_policy, update_policy, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m reward \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mrewards\n\u001b[1;32m     20\u001b[0m mean_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mmean(reward, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mPolicyTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[run, episode] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhmap_visits[run, current_state] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGridWorld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_col_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_cols\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[run, episode] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m     30\u001b[0m     next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(current_state, current_action)\n\u001b[1;32m     31\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_policy\u001b[38;5;241m.\u001b[39mselect_action(next_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[run])\n",
      "Cell \u001b[0;32mIn[6], line 221\u001b[0m, in \u001b[0;36mGridWorld.row_col_to_seq\u001b[0;34m(row_col, num_cols)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m next_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[next_state]\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrow_col_to_seq\u001b[39m(row_col, num_cols):\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#Converts state number to row_column format\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row_col[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m num_cols \u001b[38;5;241m+\u001b[39m row_col[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    226\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseq_to_col_row\u001b[39m(seq, num_cols): \n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m#Converts row_column format to state number\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = get_env('s1', wind=False, p_good_transition=1.0)\n",
    "optimal_hyperparams = reward_grid_search(env, alphas, gammas, epsilons, tau, model=\"sarsa\")\n",
    "if \"tau\" in list(optimal_hyperparams.keys()):\n",
    "    # Softmax is Best\n",
    "    exploration_policy = SoftmaxPolicy(tau=optimal_hyperparams[\"tau\"], actions=np.arange(env.num_actions))\n",
    "elif \"epsilon\" in list(optimal_hyperparams.keys()):\n",
    "    # Epsilon Greedy is Best\n",
    "    exploration_policy = EpGreedyPolicy(epsilon=optimal_hyperparams[\"epsilon\"], actions=np.arange(env.num_actions))\n",
    "update_policy = SARSAUpdate(alpha=optimal_hyperparams[\"alpha\"], gamma=optimal_hyperparams[\"gamma\"])\n",
    "trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "trainer.train()\n",
    "\n",
    "# Plotting \n",
    "trainer.plot_reward()\n",
    "trainer.plot_steps()\n",
    "trainer.plot_visits()\n",
    "trainer.plot_Q()\n",
    "trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs:   0%|          | 0/5 [00:00<?, ?it/s]/var/folders/vn/q3t9y55d35v26w9jqv0sf6p40000gn/T/ipykernel_2051/317266452.py:33: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.Q[run, current_state, current_action] = self.update_policy.update(self.Q[run], current_state, current_action, next_state, next_action, reward)\n",
      "/var/folders/vn/q3t9y55d35v26w9jqv0sf6p40000gn/T/ipykernel_2051/317266452.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.rewards[run, episode] += reward\n",
      "Training Runs: 100%|██████████| 5/5 [00:31<00:00,  6.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# env = get_env('s1', wind=False)\n",
    "# exploration_policy = SoftmaxPolicy(tau=0.0001, actions=np.arange(env.num_actions))\n",
    "# update_policy = SARSAUpdate(alpha=0.1, gamma=0.8)\n",
    "# trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.plot_reward()\n",
    "# trainer.plot_steps()\n",
    "# trainer.plot_visits()\n",
    "# trainer.plot_Q()\n",
    "trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
