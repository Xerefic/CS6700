{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Reinforcement Learning\n",
    "## Programming Assignment 1\n",
    "\n",
    "Submitted by:\n",
    "- Archish S (ME20B032)\n",
    "- Vinayak Gupta (EE20B152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Parameters of the Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_columns = 10\n",
    "start_state = {\n",
    "    \"s1\": np.array([[0, 4]]),\n",
    "    \"s2\": np.array([[3, 6]]),\n",
    "}\n",
    "obstructions = np.array([[0, 7], [1, 1], [1, 2], [1, 3], [1, 7], [2, 1], [2, 3],\n",
    "                        [2, 7], [3, 1], [3, 3], [3, 5], [4, 3], [4, 5], [4, 7],\n",
    "                        [5, 3], [5, 7], [5, 9], [6, 3], [6, 9], [7, 1], [7, 6],\n",
    "                        [7, 7], [7, 8], [7, 9], [8, 1], [8, 5], [8, 6], [9, 1]])\n",
    "\n",
    "bad_states = np.array([[1, 9], [4, 2], [4, 4], [7, 5], [9, 9]])\n",
    "restart_states = np.array([[3, 7], [8, 2]])\n",
    "goal_states = np.array([[0, 9], [2, 2], [8, 7]])\n",
    "\n",
    "step_reward = -1\n",
    "goal_reward = 10\n",
    "bad_state_reward = -6\n",
    "restart_state_reward = -100\n",
    "\n",
    "p_good_transition = 1\n",
    "bias = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnv):\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind=False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward=None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = GridWorld.row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = GridWorld.row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = GridWorld.row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states, self.num_states, self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1, 2, 1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col), 1)==0):\n",
    "                        next_state = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state, :, :] = 0\n",
    "                        self.P[state, next_state, :] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2, 3, 1, 0]\n",
    "        right = [3, 2, 0, 1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1, 1, 0, 0]\n",
    "        col_change = [0, 0, -1, 1]\n",
    "        row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0, 0] += row_change[direction]\n",
    "        row_col[0, 1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the gridworld with the start, goal, and bad states.\n",
    "        Mark X for Obstructions, G for Goal, B for Bad, and S for Start\n",
    "        \"\"\"\n",
    "        grid = np.zeros((self.num_rows, self.num_cols))\n",
    "        if self.obs_states is not None:\n",
    "            for i in range(self.obs_states.shape[0]):\n",
    "                grid[self.obs_states[i, 0], self.obs_states[i, 1]] = 1\n",
    "        for i in range(self.goal_states.shape[0]):\n",
    "            grid[self.goal_states[i, 0], self.goal_states[i, 1]] = -1\n",
    "        for i in range(self.bad_states.shape[0]):\n",
    "            grid[self.bad_states[i, 0], self.bad_states[i, 1]] = 2\n",
    "        for i in range(self.restart_states.shape[0]):\n",
    "            grid[self.restart_states[i, 0], self.restart_states[i, 1]] = 3\n",
    "        grid[self.start_state[0, 0], self.start_state[0, 1]] = -2\n",
    "        sns.heatmap(grid, annot=False, cmap=\"coolwarm\", cbar=False, linewidths=0.5, linecolor='black')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                if grid[i, j] == -1:\n",
    "                    plt.text(j+0.5, i+0.5, 'G', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 2:\n",
    "                    plt.text(j+0.5, i+0.5, 'B', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 3:\n",
    "                    plt.text(j+0.5, i+0.5, 'R', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == -2:\n",
    "                    plt.text(j+0.5, i+0.5, 'S', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 1:\n",
    "                    plt.text(j+0.5, i+0.5, 'X', ha='center', va='center', fontsize=10)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if (self.wind and np.random.random() < 0.4):\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]\n",
    "\n",
    "    @staticmethod\n",
    "    def row_col_to_seq(row_col, num_cols):\n",
    "        #Converts state number to row_column format\n",
    "        return row_col[:, 0] * num_cols + row_col[:, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def seq_to_col_row(seq, num_cols): \n",
    "        #Converts row_column format to state number\n",
    "        r = floor(seq / num_cols)\n",
    "        c = seq - r * num_cols\n",
    "        return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the environment given the specific setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(state, wind=False, p_good_transition=1.0):\n",
    "\n",
    "    # Initialising an Empty Grid World Environment\n",
    "    gw = GridWorld(         \n",
    "        num_rows=num_rows,\n",
    "        num_cols=num_columns,\n",
    "        start_state=start_state[state],\n",
    "        goal_states=goal_states,\n",
    "        wind=wind\n",
    "    )   \n",
    "\n",
    "    # Adding the Obstructions, Bad States and Restart States\n",
    "    gw.add_obstructions(\n",
    "        obstructed_states=obstructions,\n",
    "        bad_states=bad_states,\n",
    "        restart_states=restart_states\n",
    "    )\n",
    "\n",
    "    # Initialising the probability p and bias\n",
    "    gw.add_transition_probability(\n",
    "        p_good_transition=p_good_transition,\n",
    "        bias=bias\n",
    "    )\n",
    "\n",
    "    # Setting up the rewards for each action possible\n",
    "    gw.add_rewards(\n",
    "        step_reward=step_reward,\n",
    "        goal_reward=goal_reward,\n",
    "        bad_state_reward=bad_state_reward,\n",
    "        restart_state_reward=restart_state_reward\n",
    "    )\n",
    "    \n",
    "    # Creating the Environment for the agent to play\n",
    "    env = gw.create_gridworld()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Policy Definitions\n",
    "This section introduces the essential policies for selecting actions, such as the greedy and softmax policies. These policies determine the next action of the RL agent by considering the state and the corresponding Q-values of each action.\n",
    "\n",
    "Initially, we establish an abstract BasePolicy() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, a deterministic greedy policy is implemented, selecting the action with the highest Q-value. This policy aids in evaluating the effectiveness of the learned policy, a crucial step in hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'greedy'\n",
    "\n",
    "    # Actions: The available actions for each state.\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    # State: The identifier of the state for which an action is to be determined.\n",
    "    # Action Values: A two-dimensional array holding predicted action values for each (state, action) pair.\n",
    "    # The array size is |S| * 4, where |S| represents the total number of states and 4 represents the total number of actions.\n",
    "    def select_action(self, state, action_values):\n",
    "        # Choose the arm with the highest value according to the action value function.\n",
    "        return self.actions[np.argmax(action_values[state, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-Greedy Policy\n",
    "\n",
    "The $\\varepsilon$-greedy policy defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    \\underset{a \\in A(s)}{\\arg\\max} \\; Q(s, a) & \\text{with probability } 1 - \\varepsilon \\\\\n",
    "    \\textrm{random choice} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\varepsilon$: The probability of choosing a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpGreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'ep-greedy ep:{self.epsilon}'\n",
    "    # epsilon: The value of epsilon for epsilon-greedy selection.\n",
    "    # Actions: The available actions for each state.\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "    \n",
    "    # State: The identifier of the state for which an action is to be determined.\n",
    "    # Action Values: A two-dimensional array holding predicted action values for each (state, action) pair.\n",
    "    # The array size is |S| * 4, where |S| represents the total number of states and 4 represents the total number of actions.\n",
    "    def select_action(self, state, action_values):\n",
    "\n",
    "        # Deciding whether to explore or exploit\n",
    "        # If exploiting, opt for the arm yielding the highest value according to the action value function; otherwise, resort to selecting a random arm.\n",
    "        if np.random.binomial(1, 1-self.epsilon):\n",
    "            return self.actions[np.argmax(action_values[state, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Policy\n",
    "\n",
    "The softmax policy is defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    a_1 & \\text{with probability } \\mathcal{P}(1) \\\\\n",
    "    a_2 & \\text{with probability } \\mathcal{P}(2) \\\\\n",
    "    \\vdots  & \\vdots \\\\\n",
    "    a_n & \\text{with probability } \\mathcal{P}(n)\n",
    "\\end{cases}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathcal{P}(a) = \\dfrac{e^{Q(s, a) / \\tau}}{\\sum\\limits_{i=1}^{n} e^{Q(s, i) / \\tau}}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\tau$: The temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class SoftmaxPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'softmax tau:{self.tau}'\n",
    "    # Beta: The temperature parameter utilized by the softmax function.\n",
    "    # Actions: The available actions for each state.\n",
    "    def __init__(self, tau, actions):\n",
    "        self.tau = tau\n",
    "        self.actions = actions\n",
    "\n",
    "    # State: The identifier of the state for which an action is to be determined.\n",
    "    # Action Values: A two-dimensional array holding predicted action values for each (state, action) pair.\n",
    "    # The array size is |S| * 4, where |S| represents the total number of states and 4 represents the total number of actions.\n",
    "    def select_action(self, state, action_values):\n",
    "        # Sample from the softmax distribution to select an arm using the temperature parameter beta.\n",
    "        return np.random.choice(self.actions, p = softmax(action_values[state, :]/self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Policy Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section outlines essential action selection policies such as SARSA and QLearning. These policies are employed to update the Q-values according to the actions taken by the RL agent, as well as the state and Q-values of each action.\n",
    "\n",
    "Initially, an abstract BaseUpdate() class is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "The update rule for SARSA:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'sarsa alpha:{self.alpha} gamma:{self.gamma}'\n",
    "\n",
    "    # alpha - learning rate\n",
    "    # gamma - discount factor\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    # Update the Q-value for a given state-action pair using the Q-learning algorithm.\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        # Parameters:\n",
    "        # Q: Dictionary representing the Q-table.\n",
    "        # state: Current state.\n",
    "        # action: Action taken in the current state.\n",
    "        # next_state: Next state resulting from the action taken.\n",
    "        # next_action: Action taken in the next state.\n",
    "        # reward: Reward received from taking the action in the current state.\n",
    "\n",
    "        # Returns:\n",
    "        # Updated Q-value for the state-action pair.\n",
    "\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * Q[next_state, next_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "The update rule for Q-Learning:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'q-learning alpha:{self.alpha} gamma:{self.gamma}'\n",
    "\n",
    "    # alpha - learning rate\n",
    "    # gamma - discount factor\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    # Update the Q-value for a given state-action pair using the Q-learning algorithm.\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        # Parameters:\n",
    "        # Q: Dictionary representing the Q-table.\n",
    "        # state: Current state.\n",
    "        # action: Action taken in the current state.\n",
    "        # next_state: Next state resulting from the action taken.\n",
    "        # reward: Reward received from taking the action in the current state.\n",
    "\n",
    "        # Returns:\n",
    "        # Updated Q-value for the state-action pair.\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * np.max(Q[next_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivotal aspect of the learning process lies within the subsequent iterator class. This class fundamentally facilitates generalized policy iteration utilizing designated update and exploration policies. Additionally, it incorporates various utility methods aiding in result visualization, contributing to a comprehensive analysis of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyTrainer:\n",
    "    def __init__(self, env, exploration_policy, update_policy, episodes, runs, experiment):\n",
    "       \n",
    "        # Initializes the PolicyTrainer class.\n",
    "\n",
    "        # Parameters:\n",
    "        # env: Environment object.\n",
    "        # exploration_policy: Exploration policy object.\n",
    "        # update_policy (object): Update policy object.\n",
    "        # episodes (int): Number of episodes for training.\n",
    "        # runs (int): Number of runs for training.\n",
    "        # experiment (str): Name of the experiment.\n",
    "\n",
    "        self.experiment = experiment\n",
    "        self.env = env\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.update_policy = update_policy\n",
    "        self.episodes = episodes\n",
    "        self.runs = runs\n",
    "\n",
    "        # Initialize arrays to store training statistics\n",
    "        self.steps = np.zeros((runs, episodes))\n",
    "        self.rewards = np.zeros((runs, episodes))\n",
    "        self.Q = np.zeros((runs, env.num_states, env.num_actions))\n",
    "        self.hmap_visits = np.zeros((runs, env.num_states))\n",
    "        self.hmap_Q = np.zeros((runs, env.num_states))\n",
    "\n",
    "    def train(self):\n",
    "        # Trains the policy using Q-learning algorithm.\n",
    "        np.random.seed(32+152)\n",
    "\n",
    "        for run in tqdm.trange(self.runs, desc='Training Runs'):\n",
    "            for episode in range(self.episodes):\n",
    "                \n",
    "                # Initialize starting state and action\n",
    "                current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "                current_action = self.exploration_policy.select_action(current_state, self.Q[run])\n",
    "\n",
    "                # Initialize episode statistics\n",
    "                self.steps[run, episode] = 0\n",
    "                self.rewards[run, episode] = 0\n",
    "                self.hmap_visits[run, current_state] += 1\n",
    "\n",
    "                # Execute episode until termination or maximum steps reached\n",
    "                while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and self.steps[run, episode] < 100:\n",
    "                    \n",
    "                    # Take action and observe next state and reward\n",
    "                    next_state, reward = self.env.step(current_state, current_action)\n",
    "                    next_action = self.exploration_policy.select_action(next_state, self.Q[run])\n",
    "\n",
    "                    # Update Q-value\n",
    "                    self.Q[run, current_state, current_action] = self.update_policy.update(self.Q[run], current_state, current_action, next_state, next_action, reward)\n",
    "\n",
    "                    # Update visit count for next state\n",
    "                    if current_state != next_state:\n",
    "                        self.hmap_visits[run, next_state] += 1\n",
    "\n",
    "                    # Transition to next state-action pair\n",
    "                    current_state = next_state\n",
    "                    current_action = next_action\n",
    "\n",
    "                    # Update episode statistics\n",
    "                    self.steps[run, episode] += 1\n",
    "                    self.rewards[run, episode] += reward\n",
    "\n",
    "                # Check if the episode terminated without reaching the goal\n",
    "                if current_state not in list(GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols)):\n",
    "                    # self.steps[run, episode] = np.inf\n",
    "                    pass\n",
    "            \n",
    "            # Calculate the maximum Q-value for each state after each run\n",
    "            for state in range(self.env.num_states):\n",
    "                self.hmap_Q[run, state] = np.max(self.Q[run, state, :])\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Evaluates the learned policy.\n",
    "        \n",
    "        # Returns:\n",
    "        # tuple: Tuple containing rewards, steps, and path.\n",
    "\n",
    "        Q = np.mean(self.Q, axis=0)\n",
    "        policy = GreedyPolicy(np.arange(self.env.num_actions))\n",
    "        hmap_visits = np.zeros(self.env.num_states)\n",
    "        hmap_visits[GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)] = 1\n",
    "\n",
    "        current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "        current_action = policy.select_action(current_state, Q)\n",
    "\n",
    "        steps = 0\n",
    "        rewards = 0\n",
    "\n",
    "        path = [current_state]\n",
    "        while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and steps < 100:\n",
    "                \n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = policy.select_action(next_state, Q)\n",
    "    \n",
    "                hmap_visits[next_state] = 1\n",
    "    \n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "    \n",
    "                steps += 1\n",
    "                rewards += reward\n",
    "                path.append(current_state)\n",
    "\n",
    "        return rewards, steps, path\n",
    "\n",
    "\n",
    "    def plot_policy(self):\n",
    "        # Plots the learned policy.\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy_' if self.env.wind else '_clear_'\n",
    "        p_observation = str(self.env.p_good_trans)\n",
    "        name = self.experiment + '_' + self.update_policy.name + start_state + wind_state + \"p:\" + p_observation + '_' + self.exploration_policy.name + '_policy.pdf'\n",
    "\n",
    "        _, _, path = self.evaluate()\n",
    "        \n",
    "        plt.title(\"Learnt Policy\")\n",
    "        # hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        self.env.plot()\n",
    "        plt.plot([x % self.env.num_cols + 0.5 for x in path], [x // self.env.num_cols + 0.5 for x in path], 'r--')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "\n",
    "    def plot_reward(self):\n",
    "        # Plots the reward per episode.\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy_' if self.env.wind else '_clear_'\n",
    "        p_observation = str(self.env.p_good_trans)\n",
    "        name = self.experiment + '_' + self.update_policy.name + start_state + wind_state + \"p:\" + p_observation + '_' + self.exploration_policy.name + '_reward.pdf'\n",
    "\n",
    "        plt.title(f\"Reward per Episode: Avg:{round(np.mean(self.rewards), 3)}, Max:{round(np.max(self.rewards), 3)}\")\n",
    "\n",
    "        plt.plot(self.rewards.mean(axis=0), 'r')\n",
    "        plt.fill_between(range(self.episodes), self.rewards.mean(axis=0) - self.rewards.std(axis=0), self.rewards.mean(axis=0) + self.rewards.std(axis=0), alpha=0.2, color='r')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_steps(self):\n",
    "        # Plots the steps per episode.\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy_' if self.env.wind else '_clear_'\n",
    "        p_observation = str(self.env.p_good_trans)\n",
    "        name = self.experiment + '_' + self.update_policy.name + start_state + wind_state + \"p:\" + p_observation + '_' + self.exploration_policy.name + '_steps.pdf'\n",
    "\n",
    "        plt.title(f\"Steps per Episode: Avg:{round(np.mean(self.steps), 3)}, Max:{round(np.max(self.steps), 3)}, Min:{round(np.min(self.steps), 3)}\")\n",
    "        \n",
    "        plt.plot(self.steps.mean(axis=0), 'b')\n",
    "        plt.fill_between(range(self.episodes), self.steps.mean(axis=0) - self.steps.std(axis=0), self.steps.mean(axis=0) + self.steps.std(axis=0), alpha=0.2, color='b')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_visits(self):\n",
    "        # Plots the state visit counts.\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy_' if self.env.wind else '_clear_'\n",
    "        p_observation = str(self.env.p_good_trans)\n",
    "        name = self.experiment + '_' + self.update_policy.name + start_state + wind_state + \"p:\" + p_observation + '_' + self.exploration_policy.name + '_heatmap.pdf'\n",
    "\n",
    "        plt.title(\"State Visits\")\n",
    "        hmap_visits = np.mean(self.hmap_visits, axis=0)\n",
    "        hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_Q(self):\n",
    "        # Plots the Q-values.\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy_' if self.env.wind else '_clear_'\n",
    "        p_observation = str(self.env.p_good_trans)\n",
    "        name = self.experiment + '_' + self.update_policy.name + start_state + wind_state + \"p:\" + p_observation + '_' + self.exploration_policy.name + '_Q.pdf'\n",
    "\n",
    "        plt.title(f\"Q Value: Avg:{round(np.mean(self.hmap_Q), 3)}, Max:{round(np.max(self.hmap_Q), 3)}, Min:{round(np.min(self.hmap_Q), 3)}\")\n",
    "        Q = np.mean(self.hmap_Q, axis=0)\n",
    "        hmap = sns.heatmap(Q.reshape(self.env.num_rows, self.env.num_cols), annot=False, linewidths=0.5, linecolor='black', cmap=\"YlOrBr\")\n",
    "\n",
    "        for i in range(self.env.num_rows):\n",
    "            for j in range(self.env.num_cols):\n",
    "                if [i, j] in self.env.obs_states.tolist():\n",
    "                    plt.text(j+0.5, i+0.5, 'X', ha='center', va='center', fontsize=10)\n",
    "                    continue\n",
    "                if [i, j] in self.env.goal_states.tolist():\n",
    "                    plt.text(j+0.5, i+0.5, 'G', ha='center', va='center', fontsize=10)\n",
    "                    continue\n",
    "                if [i, j] in self.env.bad_states.tolist():\n",
    "                    plt.text(j+0.5, i+0.5, 'B', ha='center', va='center', fontsize=10)\n",
    "                if [i, j] in self.env.restart_states.tolist():\n",
    "                    plt.text(j+0.5, i+0.5, 'R', ha='center', va='center', fontsize=10)\n",
    "                if [i, j] in self.env.start_state.tolist():\n",
    "                    plt.text(j+0.5, i+0.5, 'S', ha='center', va='center', fontsize=10)\n",
    "\n",
    "                state = GridWorld.row_col_to_seq(np.array([[i, j]]), self.env.num_cols)[0]\n",
    "                action = np.argmax(self.Q[0, state, :])\n",
    "\n",
    "                if action == 0:\n",
    "                    plt.arrow(j+0.5, i+0.5+0.2, 0, -0.4, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "                elif action == 1:\n",
    "                    plt.arrow(j+0.5, i+0.5-0.2, 0, 0.4, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "                elif action == 2:\n",
    "                    plt.arrow(j+0.5+0.2, i+0.5, -0.4, 0, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "                elif action == 3:\n",
    "                    plt.arrow(j+0.5-0.2, i+0.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "        \n",
    "\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_grid_search(env, alphas, gammas, epsilons, taus, model = 'sarsa', policy = 'epsilon', experiment = 'experiment'):\n",
    "  \n",
    "    # Perform a grid search over hyperparameters to find the optimal combination that maximizes the reward.\n",
    "\n",
    "    # Parameters:\n",
    "    # env: Environment object.\n",
    "    # alphas: List of learning rates to search over.\n",
    "    # gammas: List of discount factors to search over.\n",
    "    # epsilons: List of exploration rates to search over.\n",
    "    # taus: List of temperature parameters for softmax policy to search over.\n",
    "    # model: Type of model to use, either 'sarsa' or 'qlearning'.\n",
    "    # policy: Type of exploration policy to use, either 'epsilon' or 'softmax'.\n",
    "    # experiment: Name of the experiment.\n",
    "\n",
    "    # Returns:\n",
    "    # tuple: Tuple containing optimal hyperparameters and the corresponding trainer object.\n",
    "\n",
    "    optimal_regret = np.inf\n",
    "    best_reward = - np.inf\n",
    "    optimal_hyperparams = {}\n",
    "\n",
    "    if policy == \"softmax\":\n",
    "    # Softmax exploration policy\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for tau in taus:\n",
    "                    print(f\"The current set of Hyperparams: alpha = {alpha}, gamma = {gamma}, tau = {tau}\")\n",
    "                    # Initialize update policy and exploration policy\n",
    "                    if model == \"sarsa\":\n",
    "                        update_policy = SARSAUpdate(alpha=alpha, gamma=gamma)\n",
    "                    elif model == \"qlearning\":\n",
    "                        update_policy = QLearningUpdate(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "                    exploration_policy = SoftmaxPolicy(tau=tau, actions=np.arange(env.num_actions))\n",
    "                    \n",
    "                    # Train the policy\n",
    "                    trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5, experiment=experiment)\n",
    "                    trainer.train()\n",
    "\n",
    "                    # Evaluate the trained policy\n",
    "                    greedy_reward, _, _ = trainer.evaluate()\n",
    "                    mean_reward = np.mean(np.mean(trainer.rewards, axis = 1), axis = 0)\n",
    "                    regret = greedy_reward - mean_reward\n",
    "\n",
    "                    # Update optimal hyperparameters if found a better combination\n",
    "                    if regret <= optimal_regret and best_reward <= greedy_reward:\n",
    "                        optimal_trainer = trainer\n",
    "                        optimal_regret = regret\n",
    "                        best_reward = greedy_reward\n",
    "                        optimal_hyperparams = {\n",
    "                            \"alpha\": alpha,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"tau\": tau\n",
    "                        }\n",
    "    \n",
    "    elif policy == \"epsilon\":\n",
    "    # Epsilon-greedy exploration policy\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for epsilon in epsilons:\n",
    "                    print(f\"The current set of Hyperparams: alpha = {alpha}, gamma = {gamma}, epsilon = {epsilon}\")\n",
    "                    # Initialize update policy and exploration policy\n",
    "                    if model == \"sarsa\":\n",
    "                        update_policy = SARSAUpdate(alpha=alpha, gamma=gamma)\n",
    "                    elif model == \"qlearning\":\n",
    "                        update_policy = QLearningUpdate(alpha=alpha, gamma=gamma)\n",
    "                        \n",
    "                    exploration_policy = EpGreedyPolicy(epsilon=epsilon, actions=np.arange(env.num_actions))\n",
    "\n",
    "                    # Train the policy\n",
    "                    trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5, experiment=experiment)\n",
    "                    trainer.train()\n",
    "\n",
    "                    # Evaluate the trained policy\n",
    "                    greedy_reward, _, _ = trainer.evaluate()\n",
    "                    mean_reward = np.mean(np.mean(trainer.rewards, axis = 1), axis = 0)\n",
    "                    regret = greedy_reward - mean_reward\n",
    "\n",
    "                    # Update optimal hyperparameters if found a better combination\n",
    "                    if regret <= optimal_regret and best_reward <= greedy_reward:\n",
    "                        optimal_trainer = trainer\n",
    "                        optimal_regret = regret\n",
    "                        best_reward = greedy_reward\n",
    "                        optimal_hyperparams = {\n",
    "                            \"alpha\": alpha,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"epsilon\": epsilon\n",
    "                        }\n",
    "\n",
    "    return optimal_hyperparams, optimal_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search is conducted on the following grid of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1, 0.2]\n",
    "gammas = [0.8, 0.9, 0.95, 1.0]\n",
    "epsilons = [0.01, 0.05, 0.1, 0.2]\n",
    "taus = [0.01, 0.05, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "State = s1 (0, 4) \n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 1 \n",
    "\n",
    "SARSA Algo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s1', wind=False, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"softmax\", experiment='experiment_1')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"epsilon\", experiment='experiment_1')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "\n",
    "State = s2 (3, 6) \n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 1 \n",
    "\n",
    "SARSA Algo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s2', wind=False, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"softmax\", experiment='experiment_2')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"epsilon\", experiment='experiment_2')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "\n",
    "State = s1 (0, 4)\n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 0.7\n",
    "\n",
    "SARSA Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s1', wind=False, p_good_transition=0.7)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"softmax\", experiment='experiment_3')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"epsilon\", experiment='experiment_3')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "\n",
    "State = s2 (3, 6)\n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 0.7\n",
    "\n",
    "SARSA Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s2', wind=False, p_good_transition=0.7)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"softmax\", experiment='experiment_4')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"epsilon\", experiment='experiment_4')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n",
    "\n",
    "State = s1 (0, 4)\n",
    "\n",
    "Wind = True \n",
    "\n",
    "p = 1.0\n",
    "\n",
    "SARSA Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s1', wind=True, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"softmax\", experiment='experiment_5')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"epsilon\", experiment='experiment_5')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6\n",
    "\n",
    "State = s2 (3, 6)\n",
    "\n",
    "Wind = True \n",
    "\n",
    "p = 1.0\n",
    "\n",
    "SARSA Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s2', wind=True, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"softmax\", experiment='experiment_6')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"sarsa\", policy = \"epsilon\", experiment='experiment_6')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "State = s1 (0, 4) \n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 1 \n",
    "\n",
    "Q-Learning Algo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s1', wind=False, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"softmax\", experiment='experiment_7')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"epsilon\", experiment='experiment_7')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "\n",
    "State = s2 (3, 6) \n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 1 \n",
    "\n",
    "Q-Learning Algo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s2', wind=False, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"softmax\", experiment='experiment_8')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"epsilon\", experiment='experiment_8')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "\n",
    "State = s1 (0, 4)\n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 0.7\n",
    "\n",
    "Q-Learning Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s1', wind=False, p_good_transition=0.7)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"softmax\", experiment='experiment_9')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"epsilon\", experiment='experiment_9')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "\n",
    "State = s2 (3, 6)\n",
    "\n",
    "Wind = False \n",
    "\n",
    "p = 0.7\n",
    "\n",
    "Q-Learning Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s2', wind=False, p_good_transition=0.7)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"softmax\", experiment='experiment_10')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"epsilon\", experiment='experiment_10')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n",
    "\n",
    "State = s1 (0, 4)\n",
    "\n",
    "Wind = True \n",
    "\n",
    "p = 1.0\n",
    "\n",
    "Q-Learning Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s1', wind=True, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"softmax\", experiment='experiment_11')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"epsilon\", experiment='experiment_11')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6\n",
    "\n",
    "State = s2 (3, 6)\n",
    "\n",
    "Wind = True \n",
    "\n",
    "p = 1.0\n",
    "\n",
    "Q-Learning Algo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with specific settings\n",
    "env = get_env('s2', wind=True, p_good_transition=1.0)\n",
    "\n",
    "# Perform grid search with Softmax exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"softmax\", experiment='experiment_12')\n",
    "\n",
    "# Print optimal hyperparameters found for Softmax policy\n",
    "print(f\"Optimal Hyperparameters for Softmax - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, tau: {optimal_hyperparams['tau']}\")\n",
    "\n",
    "# Plot metrics for Softmax policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()\n",
    "\n",
    "# Perform grid search with Epsilon Greedy exploration policy\n",
    "optimal_hyperparams, optimal_trainer = reward_grid_search(env, alphas, gammas, epsilons, taus, model = \"qlearning\", policy = \"epsilon\", experiment='experiment_12')\n",
    "\n",
    "# Print optimal hyperparameters found for Epsilon Greedy policy\n",
    "print(f\"Optimal Hyperparameters for Epsilon Greedy - alpha: {optimal_hyperparams['alpha']}, gamma: {optimal_hyperparams['gamma']}, epsilon: {optimal_hyperparams['epsilon']}\")\n",
    "\n",
    "# Plot metrics for Epsilon Greedy policy\n",
    "optimal_trainer.plot_reward()\n",
    "optimal_trainer.plot_steps()\n",
    "optimal_trainer.plot_visits()\n",
    "optimal_trainer.plot_Q()\n",
    "optimal_trainer.plot_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
