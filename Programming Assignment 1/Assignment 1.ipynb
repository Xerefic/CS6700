{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Reinforcement Learning\n",
    "## Programming Assignment 1\n",
    "\n",
    "Submitted by:\n",
    "- Archish S (ME20B032)\n",
    "- Vinayak Gupta (EE20B152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_columns = 10\n",
    "start_state = {\n",
    "    \"s1\": np.array([[0, 4]]),\n",
    "    \"s2\": np.array([[3, 6]]),\n",
    "}\n",
    "obstructions = np.array([[0, 7], [1, 1], [1, 2], [1, 3], [1, 7], [2, 1], [2, 3],\n",
    "                        [2, 7], [3, 1], [3, 3], [3, 5], [4, 3], [4, 5], [4, 7],\n",
    "                        [5, 3], [5, 7], [5, 9], [6, 3], [6, 9], [7, 1], [7, 6],\n",
    "                        [7, 7], [7, 8], [7, 9], [8, 1], [8, 5], [8, 6], [9, 1]])\n",
    "\n",
    "bad_states = np.array([[1, 9], [4, 2], [4, 4], [7, 5], [9, 9]])\n",
    "restart_states = np.array([[3, 7], [8, 2]])\n",
    "goal_states = np.array([[0, 9], [2, 2], [8, 7]])\n",
    "\n",
    "step_reward = -1\n",
    "goal_reward = 10\n",
    "bad_state_reward = -6\n",
    "restart_state_reward = -100\n",
    "\n",
    "p_good_transition = 0.8\n",
    "bias = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnv):\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind=False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward=None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = GridWorld.row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = GridWorld.row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = GridWorld.row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states, self.num_states, self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1, 2, 1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col), 1)==0):\n",
    "                        next_state = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state, :, :] = 0\n",
    "                        self.P[state, next_state, :] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2, 3, 1, 0]\n",
    "        right = [3, 2, 0, 1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1, 1, 0, 0]\n",
    "        col_change = [0, 0, -1, 1]\n",
    "        row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0, 0] += row_change[direction]\n",
    "        row_col[0, 1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if (self.wind and np.random.random() < 0.4):\n",
    "\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]\n",
    "\n",
    "    @staticmethod\n",
    "    def row_col_to_seq(row_col, num_cols):\n",
    "        #Converts state number to row_column format\n",
    "        return row_col[:, 0] * num_cols + row_col[:, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def seq_to_col_row(seq, num_cols): \n",
    "        #Converts row_column format to state number\n",
    "        r = floor(seq / num_cols)\n",
    "        c = seq - r * num_cols\n",
    "        return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(state, wind=False):\n",
    "    gw = GridWorld(\n",
    "        num_rows=num_rows,\n",
    "        num_cols=num_columns,\n",
    "        start_state=start_state[state],\n",
    "        goal_states=goal_states,\n",
    "        wind=wind\n",
    "    )\n",
    "    gw.add_obstructions(\n",
    "        obstructed_states=obstructions,\n",
    "        bad_states=bad_states,\n",
    "        restart_states=restart_states\n",
    "    )\n",
    "    gw.add_transition_probability(\n",
    "        p_good_transition=p_good_transition,\n",
    "        bias=bias\n",
    "    )\n",
    "    gw.add_rewards(\n",
    "        step_reward=step_reward,\n",
    "        goal_reward=goal_reward,\n",
    "        bad_state_reward=bad_state_reward,\n",
    "        restart_state_reward=restart_state_reward\n",
    "    )\n",
    "    env = gw.create_gridworld()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'greedy'\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return self.actions[np.argmax(action_values[state, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-Greedy Policy\n",
    "\n",
    "The $\\varepsilon$-greedy policy defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    \\underset{a \\in A(s)}{\\arg\\max} \\; Q(s, a) & \\text{with probability } 1 - \\varepsilon \\\\\n",
    "    \\textrm{random choice} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\varepsilon$: The probability of choosing a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpGreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'ep-greedy ep:{self.epsilon}'\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "\n",
    "        if np.random.binomial(1, 1-self.epsilon):\n",
    "            return self.actions[np.argmax(action_values[state, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Policy\n",
    "\n",
    "The softmax policy is defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    a_1 & \\text{with probability } \\mathcal{P}(1) \\\\\n",
    "    a_2 & \\text{with probability } \\mathcal{P}(2) \\\\\n",
    "    \\vdots  & \\vdots \\\\\n",
    "    a_n & \\text{with probability } \\mathcal{P}(n)\n",
    "\\end{cases}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathcal{P}(a) = \\dfrac{e^{Q(s, a) / \\tau}}{\\sum\\limits_{i=1}^{n} e^{Q(s, i) / \\tau}}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\tau$: The temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class SoftmaxPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'softmax tau:{self.tau}'\n",
    "\n",
    "    def __init__(self, tau, actions):\n",
    "        self.tau = tau\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return np.random.choice(self.actions, p = softmax(action_values[state, :]/self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "The update rule for SARSA:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'sarsa'\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * Q[next_state, next_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "The update rule for Q-Learning:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'q-learning'\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * np.max(Q[next_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyTrainer:\n",
    "    def __init__(self, env, exploration_policy, update_policy, episodes, runs):\n",
    "        self.env = env\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.update_policy = update_policy\n",
    "        self.episodes = episodes\n",
    "        self.runs = runs\n",
    "\n",
    "        self.steps = np.zeros((runs, episodes))\n",
    "        self.rewards = np.zeros((runs, episodes))\n",
    "        self.Q = np.zeros((runs, env.num_states, env.num_actions))\n",
    "        self.hmap_visits = np.zeros((runs, env.num_states))\n",
    "        self.hmap_Q = np.zeros((runs, env.num_states))\n",
    "\n",
    "    def train(self):\n",
    "        np.random.seed(32+152)\n",
    "\n",
    "        for run in tqdm.trange(self.runs, desc='Training Runs'):\n",
    "            for episode in range(self.episodes):\n",
    "\n",
    "                current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "                current_action = self.exploration_policy.select_action(current_state, self.Q[run])\n",
    "\n",
    "                self.steps[run, episode] = 0\n",
    "                self.rewards[run, episode] = 0\n",
    "                self.hmap_visits[run, current_state] += 1\n",
    "\n",
    "                while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and self.steps[run, episode] < 100:\n",
    "\n",
    "                    next_state, reward = self.env.step(current_state, current_action)\n",
    "                    next_action = self.exploration_policy.select_action(next_state, self.Q[run])\n",
    "\n",
    "                    self.Q[run, current_state, current_action] = self.update_policy.update(self.Q[run], current_state, current_action, next_state, next_action, reward)\n",
    "\n",
    "                    if current_state != next_state:\n",
    "                        self.hmap_visits[run, next_state] += 1\n",
    "\n",
    "                    current_state = next_state\n",
    "                    current_action = next_action\n",
    "\n",
    "                    self.steps[run, episode] += 1\n",
    "                    self.rewards[run, episode] += reward\n",
    "\n",
    "                if current_state not in list(GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols)):\n",
    "                    self.steps[run, episode] = np.inf\n",
    "\n",
    "            for state in range(self.env.num_states):\n",
    "                self.hmap_Q[run, state] = np.max(self.Q[run, state, :])\n",
    "\n",
    "    def plot_policy(self):\n",
    "        Q = np.mean(self.Q, axis=0)\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_policy.jpg'\n",
    "\n",
    "        policy = GreedyPolicy(np.arange(self.env.num_actions))\n",
    "        hmap_visits = np.zeros(self.env.num_states)\n",
    "        hmap_visits[GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)] = 1\n",
    "\n",
    "        current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "        current_action = policy.select_action(current_state, Q)\n",
    "\n",
    "        steps = 0\n",
    "        rewards = 0\n",
    "\n",
    "        while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and steps < 100:\n",
    "                \n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = policy.select_action(next_state, Q)\n",
    "    \n",
    "                hmap_visits[next_state] = 1\n",
    "    \n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "    \n",
    "                steps += 1\n",
    "                rewards += reward\n",
    "\n",
    "        plt.title(\"Learnt Policy\")\n",
    "        hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_reward(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_reward.jpg'\n",
    "    \n",
    "        plt.title(f\"Reward per Episode: Avg:{round(np.mean(self.rewards), 3)}, Max:{round(np.max(self.rewards), 3)}\")\n",
    "\n",
    "        plt.plot(self.rewards.mean(axis=0), 'r')\n",
    "        plt.fill_between(range(self.episodes), self.rewards.mean(axis=0) - self.rewards.std(axis=0), self.rewards.mean(axis=0) + self.rewards.std(axis=0), alpha=0.2, color='r')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        \n",
    "    def plot_steps(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_steps.jpg'\n",
    "\n",
    "        plt.title(f\"Steps per Episode: Avg:{round(np.mean(self.steps), 3)}, Max:{round(np.max(self.steps), 3)}, Min:{round(np.min(self.steps), 3)}\")\n",
    "        \n",
    "        plt.plot(self.steps.mean(axis=0), 'b')\n",
    "        plt.fill_between(range(self.episodes), self.steps.mean(axis=0) - self.steps.std(axis=0), self.steps.mean(axis=0) + self.steps.std(axis=0), alpha=0.2, color='b')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_heatmap(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_heatmap.jpg'\n",
    "\n",
    "        plt.title(\"State Visits\")\n",
    "        hmap_visits = np.mean(self.hmap_visits, axis=0)\n",
    "        hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_Q(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_Q.jpg'\n",
    "\n",
    "        plt.title(f\"State Value Function: Avg:{round(np.mean(self.hmap_Q), 3)}, Max:{round(np.max(self.hmap_Q), 3)}, Min:{round(np.min(self.hmap_Q), 3)}\")\n",
    "        Q = np.mean(self.hmap_Q, axis=0)\n",
    "        hmap = sns.heatmap(Q.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Runs: 100%|██████████| 10/10 [00:42<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "env = get_env('s1', wind=False)\n",
    "exploration_policy = SoftmaxPolicy(tau=0.5, actions=np.arange(env.num_actions))\n",
    "update_policy = SARSAUpdate(alpha=0.1, gamma=1)\n",
    "\n",
    "trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=10)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.plot_reward()\n",
    "# trainer.plot_steps()\n",
    "# trainer.plot_heatmap()\n",
    "# trainer.plot_Q()\n",
    "trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
