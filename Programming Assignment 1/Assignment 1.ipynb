{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Reinforcement Learning\n",
    "## Programming Assignment 1\n",
    "\n",
    "Submitted by:\n",
    "- Archish S (ME20B032)\n",
    "- Vinayak Gupta (EE20B152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10\n",
    "num_columns = 10\n",
    "start_state = {\n",
    "    \"s1\": np.array([[0, 4]]),\n",
    "    \"s2\": np.array([[3, 6]]),\n",
    "}\n",
    "obstructions = np.array([[0, 7], [1, 1], [1, 2], [1, 3], [1, 7], [2, 1], [2, 3],\n",
    "                        [2, 7], [3, 1], [3, 3], [3, 5], [4, 3], [4, 5], [4, 7],\n",
    "                        [5, 3], [5, 7], [5, 9], [6, 3], [6, 9], [7, 1], [7, 6],\n",
    "                        [7, 7], [7, 8], [7, 9], [8, 1], [8, 5], [8, 6], [9, 1]])\n",
    "\n",
    "bad_states = np.array([[1, 9], [4, 2], [4, 4], [7, 5], [9, 9]])\n",
    "restart_states = np.array([[3, 7], [8, 2]])\n",
    "goal_states = np.array([[0, 9], [2, 2], [8, 7]])\n",
    "\n",
    "step_reward = -1\n",
    "goal_reward = 10\n",
    "bad_state_reward = -6\n",
    "restart_state_reward = -100\n",
    "\n",
    "p_good_transition = 1\n",
    "bias = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnv):\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind=False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward=None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = GridWorld.row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = GridWorld.row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = GridWorld.row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states, self.num_states, self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1, 2, 1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col), 1)==0):\n",
    "                        next_state = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state, :, :] = 0\n",
    "                        self.P[state, next_state, :] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2, 3, 1, 0]\n",
    "        right = [3, 2, 0, 1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1, 1, 0, 0]\n",
    "        col_change = [0, 0, -1, 1]\n",
    "        row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0, 0] += row_change[direction]\n",
    "        row_col[0, 1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:, 0] > self.num_rows-1) or\n",
    "                np.any(row_col[:, 1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the gridworld with the start, goal, and bad states.\n",
    "        Mark X for Obstructions, G for Goal, B for Bad, and S for Start\n",
    "        \"\"\"\n",
    "        grid = np.zeros((self.num_rows, self.num_cols))\n",
    "        if self.obs_states is not None:\n",
    "            for i in range(self.obs_states.shape[0]):\n",
    "                grid[self.obs_states[i, 0], self.obs_states[i, 1]] = 1\n",
    "        for i in range(self.goal_states.shape[0]):\n",
    "            grid[self.goal_states[i, 0], self.goal_states[i, 1]] = -1\n",
    "        for i in range(self.bad_states.shape[0]):\n",
    "            grid[self.bad_states[i, 0], self.bad_states[i, 1]] = 2\n",
    "        for i in range(self.restart_states.shape[0]):\n",
    "            grid[self.restart_states[i, 0], self.restart_states[i, 1]] = 3\n",
    "        grid[self.start_state[0, 0], self.start_state[0, 1]] = -2\n",
    "        sns.heatmap(grid, annot=False, cmap=\"coolwarm\", cbar=False, linewidths=0.5, linecolor='black')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                if grid[i, j] == -1:\n",
    "                    plt.text(j+0.5, i+0.5, 'G', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 2:\n",
    "                    plt.text(j+0.5, i+0.5, 'B', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 3:\n",
    "                    plt.text(j+0.5, i+0.5, 'R', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == -2:\n",
    "                    plt.text(j+0.5, i+0.5, 'S', ha='center', va='center', fontsize=10)\n",
    "                if grid[i, j] == 1:\n",
    "                    plt.text(j+0.5, i+0.5, 'X', ha='center', va='center', fontsize=10)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if (self.wind and np.random.random() < 0.4):\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]\n",
    "\n",
    "    @staticmethod\n",
    "    def row_col_to_seq(row_col, num_cols):\n",
    "        #Converts state number to row_column format\n",
    "        return row_col[:, 0] * num_cols + row_col[:, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def seq_to_col_row(seq, num_cols): \n",
    "        #Converts row_column format to state number\n",
    "        r = floor(seq / num_cols)\n",
    "        c = seq - r * num_cols\n",
    "        return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(state, wind=False, p_good_transition=1.0):\n",
    "    gw = GridWorld(\n",
    "        num_rows=num_rows,\n",
    "        num_cols=num_columns,\n",
    "        start_state=start_state[state],\n",
    "        goal_states=goal_states,\n",
    "        wind=wind\n",
    "    )\n",
    "    gw.add_obstructions(\n",
    "        obstructed_states=obstructions,\n",
    "        bad_states=bad_states,\n",
    "        restart_states=restart_states\n",
    "    )\n",
    "    gw.add_transition_probability(\n",
    "        p_good_transition=p_good_transition,\n",
    "        bias=bias\n",
    "    )\n",
    "    gw.add_rewards(\n",
    "        step_reward=step_reward,\n",
    "        goal_reward=goal_reward,\n",
    "        bad_state_reward=bad_state_reward,\n",
    "        restart_state_reward=restart_state_reward\n",
    "    )\n",
    "    env = gw.create_gridworld()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'greedy'\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return self.actions[np.argmax(action_values[state, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-Greedy Policy\n",
    "\n",
    "The $\\varepsilon$-greedy policy defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    \\underset{a \\in A(s)}{\\arg\\max} \\; Q(s, a) & \\text{with probability } 1 - \\varepsilon \\\\\n",
    "    \\textrm{random choice} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\varepsilon$: The probability of choosing a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpGreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'ep-greedy ep:{self.epsilon}'\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "\n",
    "        if np.random.binomial(1, 1-self.epsilon):\n",
    "            return self.actions[np.argmax(action_values[state, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Policy\n",
    "\n",
    "The softmax policy is defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    a_1 & \\text{with probability } \\mathcal{P}(1) \\\\\n",
    "    a_2 & \\text{with probability } \\mathcal{P}(2) \\\\\n",
    "    \\vdots  & \\vdots \\\\\n",
    "    a_n & \\text{with probability } \\mathcal{P}(n)\n",
    "\\end{cases}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathcal{P}(a) = \\dfrac{e^{Q(s, a) / \\tau}}{\\sum\\limits_{i=1}^{n} e^{Q(s, i) / \\tau}}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\tau$: The temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class SoftmaxPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'softmax tau:{self.tau}'\n",
    "\n",
    "    def __init__(self, tau, actions):\n",
    "        self.tau = tau\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return np.random.choice(self.actions, p = softmax(action_values[state, :]/self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "The update rule for SARSA:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'sarsa'\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * Q[next_state, next_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "The update rule for Q-Learning:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'q-learning'\n",
    "\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * np.max(Q[next_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyTrainer:\n",
    "    def __init__(self, env, exploration_policy, update_policy, episodes, runs):\n",
    "        self.env = env\n",
    "        self.exploration_policy = exploration_policy\n",
    "        self.update_policy = update_policy\n",
    "        self.episodes = episodes\n",
    "        self.runs = runs\n",
    "\n",
    "        self.steps = np.zeros((runs, episodes))\n",
    "        self.rewards = np.zeros((runs, episodes))\n",
    "        self.Q = np.zeros((runs, env.num_states, env.num_actions))\n",
    "        self.hmap_visits = np.zeros((runs, env.num_states))\n",
    "        self.hmap_Q = np.zeros((runs, env.num_states))\n",
    "\n",
    "    def train(self):\n",
    "        np.random.seed(32+152)\n",
    "\n",
    "        for run in tqdm.trange(self.runs, desc='Training Runs'):\n",
    "            for episode in range(self.episodes):\n",
    "\n",
    "                current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "                current_action = self.exploration_policy.select_action(current_state, self.Q[run])\n",
    "\n",
    "                self.steps[run, episode] = 0\n",
    "                self.rewards[run, episode] = 0\n",
    "                self.hmap_visits[run, current_state] += 1\n",
    "\n",
    "                while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and self.steps[run, episode] < 100:\n",
    "\n",
    "                    next_state, reward = self.env.step(current_state, current_action)\n",
    "                    next_action = self.exploration_policy.select_action(next_state, self.Q[run])\n",
    "\n",
    "                    self.Q[run, current_state, current_action] = self.update_policy.update(self.Q[run], current_state, current_action, next_state, next_action, reward)\n",
    "\n",
    "                    if current_state != next_state:\n",
    "                        self.hmap_visits[run, next_state] += 1\n",
    "\n",
    "                    current_state = next_state\n",
    "                    current_action = next_action\n",
    "\n",
    "                    self.steps[run, episode] += 1\n",
    "                    self.rewards[run, episode] += reward\n",
    "\n",
    "                if current_state not in list(GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols)):\n",
    "                    self.steps[run, episode] = np.inf\n",
    "\n",
    "            for state in range(self.env.num_states):\n",
    "                self.hmap_Q[run, state] = np.max(self.Q[run, state, :])\n",
    "\n",
    "    def evaluate(self):\n",
    "        Q = np.mean(self.Q, axis=0)\n",
    "        policy = GreedyPolicy(np.arange(self.env.num_actions))\n",
    "        hmap_visits = np.zeros(self.env.num_states)\n",
    "        hmap_visits[GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)] = 1\n",
    "\n",
    "        current_state = GridWorld.row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "        current_action = policy.select_action(current_state, Q)\n",
    "\n",
    "        steps = 0\n",
    "        rewards = 0\n",
    "\n",
    "        path = [current_state]\n",
    "        while current_state not in GridWorld.row_col_to_seq(self.env.goal_states, self.env.num_cols) and steps < 100:\n",
    "                \n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = policy.select_action(next_state, Q)\n",
    "    \n",
    "                hmap_visits[next_state] = 1\n",
    "    \n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "    \n",
    "                steps += 1\n",
    "                rewards += reward\n",
    "                path.append(current_state)\n",
    "\n",
    "        return rewards, steps, path, hmap_visits\n",
    "\n",
    "\n",
    "    def plot_policy(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_policy.jpg'\n",
    "\n",
    "        _, _, path, hmap_visits = self.evaluate()\n",
    "        \n",
    "        plt.title(\"Learnt Policy\")\n",
    "        # hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        self.env.plot()\n",
    "        plt.plot([x % self.env.num_cols + 0.5 for x in path], [x // self.env.num_cols + 0.5 for x in path], 'r--')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_reward(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_reward.jpg'\n",
    "    \n",
    "        plt.title(f\"Reward per Episode: Avg:{round(np.mean(self.rewards), 3)}, Max:{round(np.max(self.rewards), 3)}\")\n",
    "\n",
    "        plt.plot(self.rewards.mean(axis=0), 'r')\n",
    "        plt.fill_between(range(self.episodes), self.rewards.mean(axis=0) - self.rewards.std(axis=0), self.rewards.mean(axis=0) + self.rewards.std(axis=0), alpha=0.2, color='r')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "        \n",
    "    def plot_steps(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_steps.jpg'\n",
    "\n",
    "        plt.title(f\"Steps per Episode: Avg:{round(np.mean(self.steps), 3)}, Max:{round(np.max(self.steps), 3)}, Min:{round(np.min(self.steps), 3)}\")\n",
    "        \n",
    "        plt.plot(self.steps.mean(axis=0), 'b')\n",
    "        plt.fill_between(range(self.episodes), self.steps.mean(axis=0) - self.steps.std(axis=0), self.steps.mean(axis=0) + self.steps.std(axis=0), alpha=0.2, color='b')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_visits(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_heatmap.jpg'\n",
    "\n",
    "        plt.title(\"State Visits\")\n",
    "        hmap_visits = np.mean(self.hmap_visits, axis=0)\n",
    "        hmap = sns.heatmap(hmap_visits.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')\n",
    "\n",
    "    def plot_Q(self):\n",
    "\n",
    "        start_state = '_s1' if (self.env.start_state == np.array([0, 4])).all() else '_s2'\n",
    "        wind_state = '_windy' if self.env.wind else '_clear'\n",
    "        name = self.update_policy.name + start_state + wind_state + self.exploration_policy.name + '_Q.jpg'\n",
    "\n",
    "        plt.title(f\"State Value Function: Avg:{round(np.mean(self.hmap_Q), 3)}, Max:{round(np.max(self.hmap_Q), 3)}, Min:{round(np.min(self.hmap_Q), 3)}\")\n",
    "        Q = np.mean(self.hmap_Q, axis=0)\n",
    "        hmap = sns.heatmap(Q.reshape(self.env.num_rows, self.env.num_cols), annot=False)\n",
    "        plt.savefig('plots/' + name, pad_inches=0.1, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env('s1', wind=False, p_good_transition=1.0)\n",
    "exploration_policy = EpGreedyPolicy(epsilon=0.1, actions=np.arange(env.num_actions))\n",
    "update_policy = SARSAUpdate(alpha=0.1, gamma=1)\n",
    "\n",
    "trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=1000, runs=2)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct hyperparameter tuning, we opt for maximizing asymptotic optimality, which entails leveraging Q-values acquired by the agent and employing a greedy action selection method. \n",
    "\n",
    "Following this approach, we establish a grid search function to determine the optimal hyperparameter set based on asymptotic optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_grid_search(env, alphas, gammas, epsilons, taus, model = 'sarsa', policy = 'epsilon'):\n",
    "    optimal_reward = - np.inf\n",
    "    best_reward = - np.inf\n",
    "    optimal_hyperparams = {}\n",
    "\n",
    "    if policy == \"softmax\":\n",
    "    # Softmax\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for tau in taus:\n",
    "                    print(f\"The current set of Hyperparams: alpha = {alpha}, gamma = {gamma}, tau = {tau}\")\n",
    "                    if model == \"sarsa\":\n",
    "                        update_policy = SARSAUpdate(alpha=alpha, gamma=gamma)\n",
    "                    elif model == \"qlearning\":\n",
    "                        update_policy = QLearningUpdate(alpha=alpha, gamma=gamma)\n",
    "        \n",
    "                    exploration_policy = SoftmaxPolicy(tau=tau, actions=np.arange(env.num_actions))\n",
    "                    trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "                    trainer.train()\n",
    "                    greedy_reward, _, _, _ = trainer.evaluate()\n",
    "                    reward = trainer.rewards\n",
    "                    mean_reward = np.mean(np.mean(reward, axis = 1), axis = 0)\n",
    "                    if optimal_reward < mean_reward and best_reward < greedy_reward:\n",
    "                        best_reward = greedy_reward\n",
    "                        optimal_reward = mean_reward\n",
    "                        optimal_hyperparams = {\n",
    "                            \"alpha\": alpha,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"tau\": tau\n",
    "                        }\n",
    "    \n",
    "    elif policy == \"epsilon\":\n",
    "    # Epsilon\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for epsilon in epsilons:\n",
    "                    print(f\"The current set of Hyperparams: alpha = {alpha}, gamma = {gamma}, epsilon = {epsilon}\")\n",
    "                    if model == \"sarsa\":\n",
    "                        update_policy = SARSAUpdate(alpha=alpha, gamma=gamma)\n",
    "                    elif model == \"qlearning\":\n",
    "                        update_policy = QLearningUpdate(alpha=alpha, gamma=gamma)\n",
    "                        \n",
    "                    exploration_policy = EpGreedyPolicy(epsilon=epsilon, actions=np.arange(env.num_actions))\n",
    "                    trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "                    trainer.train()\n",
    "                    greedy_reward, _, _, _ = trainer.evaluate()\n",
    "                    reward = trainer.rewards\n",
    "                    mean_reward = np.mean(np.mean(reward, axis = 1), axis = 0)\n",
    "                    if optimal_reward < mean_reward and best_reward < greedy_reward:\n",
    "                        best_reward = greedy_reward\n",
    "                        optimal_reward = mean_reward\n",
    "                        optimal_hyperparams = {\n",
    "                            \"alpha\": alpha,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"epsilon\": epsilon\n",
    "                        }\n",
    "\n",
    "    return optimal_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$: 0.001, 0.01, 0.1, 0.2\n",
    "\n",
    "$\\gamma$: 0.7, 0.8, 0.9, 1\n",
    "\n",
    "$\\epsilon$:  0.001, 0.01, 0.1, 0.5\n",
    "\n",
    "$\\tau$: 0.01, 0.1, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 0.2]\n",
    "gammas = [0.7, 0.8, 0.9, 1]\n",
    "epsilons = [0.001, 0.01, 0.1, 0.5]\n",
    "tau = [0.01, 0.1, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "State = s1 (0, 4) \n",
    "Wind = False \n",
    "p = 1 \n",
    "SARSA Algo \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env('s1', wind=False, p_good_transition=1.0)\n",
    "optimal_hyperparams = reward_grid_search(env, alphas, gammas, epsilons, tau, model=\"sarsa\")\n",
    "if \"tau\" in list(optimal_hyperparams.keys()):\n",
    "    # Softmax is Best\n",
    "    exploration_policy = SoftmaxPolicy(tau=optimal_hyperparams[\"tau\"], actions=np.arange(env.num_actions))\n",
    "elif \"epsilon\" in list(optimal_hyperparams.keys()):\n",
    "    # Epsilon Greedy is Best\n",
    "    exploration_policy = EpGreedyPolicy(epsilon=optimal_hyperparams[\"epsilon\"], actions=np.arange(env.num_actions))\n",
    "update_policy = SARSAUpdate(alpha=optimal_hyperparams[\"alpha\"], gamma=optimal_hyperparams[\"gamma\"])\n",
    "trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=100, runs=5)\n",
    "trainer.train()\n",
    "\n",
    "# Plotting \n",
    "trainer.plot_reward()\n",
    "trainer.plot_steps()\n",
    "trainer.plot_visits()\n",
    "trainer.plot_Q()\n",
    "trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = get_env('s1', wind=False)\n",
    "# exploration_policy = SoftmaxPolicy(tau=0.0001, actions=np.arange(env.num_actions))\n",
    "# update_policy = SARSAUpdate(alpha=0.1, gamma=0.8)\n",
    "# trainer = PolicyTrainer(env, exploration_policy, update_policy, episodes=10000, runs=5)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_reward()\n",
    "# trainer.plot_steps()\n",
    "# trainer.plot_visits()\n",
    "# trainer.plot_Q()\n",
    "trainer.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
