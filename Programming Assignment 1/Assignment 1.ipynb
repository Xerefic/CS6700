{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Reinforcement Learning\n",
    "## Programming Assignment 1\n",
    "\n",
    "Submitted by:\n",
    "- Archish S (ME20B032)\n",
    "- Vinayak Gupta (EE20B152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnv):\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = GridWorld.row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "\n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = GridWorld.row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = GridWorld.row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "\n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = GridWorld.row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = GridWorld.seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = GridWorld.row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "      return int(self.start_state_seq)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "\n",
    "            p += self.P[state, next_state, action]\n",
    "\n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if (self.wind and np.random.random() < 0.4):\n",
    "\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]\n",
    "\n",
    "    @staticmethod\n",
    "    def row_col_to_seq(row_col, num_cols):\n",
    "        #Converts state number to row_column format\n",
    "        return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def seq_to_col_row(seq, num_cols): \n",
    "        #Converts row_column format to state number\n",
    "        r = floor(seq / num_cols)\n",
    "        c = seq - r * num_cols\n",
    "        return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\varepsilon$-Greedy Policy\n",
    "\n",
    "The $\\varepsilon$-greedy policy defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    \\underset{a \\in A(s)}{\\arg\\max} \\; Q(s, a) & \\text{with probability } 1 - \\varepsilon \\\\\n",
    "    \\textrm{random choice} & \\text{with probability } \\varepsilon\n",
    "\\end{cases}\n",
    "$$.\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\varepsilon$: The probability of choosing a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpGreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'ep-greedy ep:{self.epsilon}'\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "\n",
    "        if np.random.binomial(1, 1-self.epsilon):\n",
    "            return self.actions[np.argmax(action_values[state, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Policy\n",
    "\n",
    "The softmax policy is defined as\n",
    "$$\n",
    "\\textrm{next\\_action} = \\begin{cases}\n",
    "    a_1 & \\text{with probability } \\mathcal{P}(1) \\\\\n",
    "    a_2 & \\text{with probability } \\mathcal{P}(2) \\\\\n",
    "    \\vdots  & \\vdots \\\\\n",
    "    a_n & \\text{with probability } \\mathcal{P}(n)\n",
    "\\end{cases}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\mathcal{P}(a) = \\dfrac{e^{Q(s, a) / \\tau}}{\\sum\\limits_{i=1}^{n} e^{Q(s, i) / \\tau}}\n",
    "$$.\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\tau$: The temperature parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'softmax tau:{self.temperature}'\n",
    "\n",
    "    def __init__(self, tau, actions):\n",
    "        self.tau = tau\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        action_probs = np.exp(action_values[state, :] / self.tau)\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return np.random.choice(self.actions, p=action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'greedy'\n",
    "\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, state, action_values):\n",
    "        return self.actions[np.argmax(action_values[state, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "The update rule for SARSA:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'sarsa'\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, policy):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "    def update(self, Q, state, action, reward, next_state, next_action):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * Q[next_state, next_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "The update rule for Q-Learning:\n",
    "$$\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\gamma$: The discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningUpdate(BaseUpdate):\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'q-learning'\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, policy):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "    def update(self, Q, state, action, next_state, next_action, reward):\n",
    "        return Q[state, action] + self.alpha * (reward + self.gamma * np.max(Q[next_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
