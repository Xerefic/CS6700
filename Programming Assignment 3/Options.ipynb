{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6700: Reinforcement Learning\n",
    "## Programming Assignment 3\n",
    "\n",
    "Submitted by:\n",
    "- Archish S (ME20B032)\n",
    "- Vinayak Gupta (EE20B152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Markov Decision Process\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import glob\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "    \"font.size\": 10\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tqdm\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "state = env.reset()\n",
    "\n",
    "# Current State\n",
    "print(\"Current State:\", env.s)\n",
    "\n",
    "# Observation Space\n",
    "locations = [\"red\", \"green\", \"yellow\", \"blue\"]\n",
    "print (\"Number of states:\", env.observation_space.n)\n",
    "\n",
    "# Action Space\n",
    "actions = [\"south\", \"north\", \"east\", \"west\", \"pick\", \"drop\"]\n",
    "print (\"Number of actions:\", env.action_space.n)\n",
    "\n",
    "# Transition \n",
    "action = random.randint(0, len(actions)-1)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(\"Action:\", actions[action])\n",
    "print(\"Next State:\", next_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "\n",
    "print(\"Decoded Current State:\", list(env.decode(state)))\n",
    "print(\"Decoded Next State:\", list(env.decode(next_state)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_runs = 5\n",
    "max_episodes = 1000\n",
    "max_steps = 500\n",
    "\n",
    "epsilon_max = 0.99\n",
    "epsilon_min = 0.01\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "\n",
    "nX = 5; nY = 5; nPas = 5; nDrop = 4\n",
    "nS = nX * nY * nPas * nDrop\n",
    "\n",
    "targets = {0: [0, 0], 1: [0, 4], 2: [4, 0], 3: [4, 3]}\n",
    "nO = 4\n",
    "nA = 6\n",
    "\n",
    "assert nS == env.observation_space.n\n",
    "assert nA == env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(q_values, state, epsilon):\n",
    "    if q_values[state].any() and random.random() > epsilon:\n",
    "        return np.argmax(q_values[state])\n",
    "\n",
    "    choice = random.randint(0, q_values.shape[-1] - 1)  \n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Option(env, state, Q, goal, epsilon=0.1):\n",
    "    optdone = False\n",
    "    x, y, pas, drop = env.decode(state)\n",
    "\n",
    "    if (x == targets[goal][0] and y == targets[goal][1]):\n",
    "        optdone = True \n",
    "        if pas == goal:\n",
    "            optact = 4\n",
    "        elif drop == goal:\n",
    "            optact = 5 \n",
    "        else:   \n",
    "            optact = 1 if (goal in [0, 1]) else 0\n",
    "    else:\n",
    "        optact = egreedy_policy(Q, state, epsilon=epsilon) \n",
    "    return [optact, optdone]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMDP Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_SMDP = np.zeros((max_runs, nS, nA + nO))\n",
    "Q_SMDP_Options = dict([(i, np.zeros((max_runs, nS, nA))) for i in range(nO)])\n",
    "\n",
    "updates_SMDP = []\n",
    "\n",
    "run_rewards = []\n",
    "for run in range(max_runs):\n",
    "\n",
    "    episode_updates = []\n",
    "    episode_rewards = []\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        step_updates = []\n",
    "        step_rewards = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            epsilon = max(epsilon_min, epsilon_max - (epsilon_max - epsilon_min) * episode / max_episodes)\n",
    "\n",
    "            x, y, pas, drop = env.decode(state)\n",
    "            action = egreedy_policy(Q_SMDP[run], state, epsilon=epsilon)\n",
    "\n",
    "            if action in [0, 1, 2, 3, 4, 5]:\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                Q_SMDP[run, state, action] = Q_SMDP[run, state, action] + alpha * (reward + gamma * np.max(Q_SMDP[run, next_state]) - Q_SMDP[run, state, action])\n",
    "                \n",
    "                step_rewards += reward\n",
    "                step_updates.append(step)\n",
    "                step += 1\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            else:\n",
    "                start = state\n",
    "                goal = action - 6\n",
    "\n",
    "                optrewards = []\n",
    "                optdone, optsteps = False, 0\n",
    "\n",
    "                while not optdone:\n",
    "                    optact, optdone = Option(env, state, Q_SMDP_Options[goal][run], goal, epsilon=epsilon)\n",
    "                    next_state, reward, done, info = env.step(optact)\n",
    "\n",
    "                    Q_SMDP_Options[goal][run, state, optact] = Q_SMDP_Options[goal][run, state, optact] + alpha * (reward + gamma * np.max(Q_SMDP_Options[goal][run, next_state]) - Q_SMDP_Options[goal][run, state, optact])\n",
    "\n",
    "                    optrewards.append(gamma**optsteps * reward)\n",
    "                    optsteps += 1\n",
    "                    state = next_state\n",
    "\n",
    "                optrewards = sum(optrewards)\n",
    "                Q_SMDP[run, start, action] = Q_SMDP[run, start, action] + alpha * (optrewards - gamma**optsteps * np.max(Q_SMDP[run, next_state]) - Q_SMDP[run, start, action])\n",
    "\n",
    "                step_rewards += optrewards\n",
    "                step_updates.append(step + optsteps)\n",
    "                step += optsteps\n",
    "\n",
    "\n",
    "        episode_rewards.append(step_rewards)\n",
    "        episode_updates.append(step_updates)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Run: {run+1}/{max_runs}, Episode: {episode+1}/{max_episodes}: {step_rewards}\")\n",
    "\n",
    "    run_rewards.append(episode_rewards)\n",
    "    updates_SMDP.append(episode_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-Option Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_Intra = np.zeros((max_runs, nS, nA))\n",
    "Q_Intra_Options = dict([(i, np.zeros((max_runs, nS, nA))) for i in range(nO)])\n",
    "\n",
    "updates_Intra = []\n",
    "\n",
    "run_rewards = []\n",
    "for run in range(max_runs):\n",
    "\n",
    "    episode_updates = []\n",
    "    episode_rewards = []\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        step_updates = []\n",
    "        step_rewards = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            epsilon = max(epsilon_min, epsilon_max - (epsilon_max - epsilon_min) * episode / max_episodes)\n",
    "\n",
    "            x, y, pas, drop = env.decode(state)\n",
    "            action = egreedy_policy(Q_Intra[run], state, epsilon=epsilon)\n",
    "\n",
    "            if action in [0, 1, 2, 3, 4, 5]:\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                Q_Intra[run, state, action] = Q_Intra[run, state, action] + alpha * (reward + gamma * np.max(Q_Intra[run, next_state]) - Q_Intra[run, state, action])\n",
    "                \n",
    "                step_rewards += reward\n",
    "                step_updates.append(step)\n",
    "                step += 1\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            else:\n",
    "                start = state\n",
    "                goal = action - 6\n",
    "\n",
    "                optrewards = []\n",
    "                optdone, optsteps = False, 0\n",
    "\n",
    "                while not optdone:\n",
    "                    optact, optdone = Option(env, state, Q_Intra_Options[goal][run], goal, epsilon=epsilon)\n",
    "                    next_state, reward, done, info = env.step(optact)\n",
    "                    next_x, next_y, next_pas, next_drop = env.decode(next_state)\n",
    "\n",
    "                    Q_Intra_Options[goal][run, state, optact] = Q_Intra_Options[goal][run, state, optact] + alpha * (reward + gamma * np.max(Q_Intra_Options[goal][run, next_state]) - Q_Intra_Options[goal][run, state, optact])\n",
    "\n",
    "                    for G in range(nO):\n",
    "                        if np.argmax(Q_Intra_Options[G][run, next_state]) != optact and G != goal:\n",
    "                            continue\n",
    "                            \n",
    "                        if next_x == targets[G][0] and next_y == targets[G][1]:\n",
    "                            Q_Intra[run, state, G + 6] = Q_Intra[run, state, G + 6] + alpha * (reward + gamma * np.max(Q_Intra[run, next_state]) - Q_Intra[run, state, G + 6])\n",
    "                        else:\n",
    "                            Q_Intra[run, state, G + 6] = Q_Intra[run, state, G + 6] + alpha * (reward + gamma * Q_Intra[run, next_state, G + 6] - Q_Intra[run, state, G + 6])\n",
    "                    \n",
    "                    step_updates.append(step + optsteps)\n",
    "\n",
    "                    optrewards.append(gamma**optsteps * reward)\n",
    "                    optsteps += 1\n",
    "                    state = next_state\n",
    "\n",
    "                optrewards = sum(optrewards)\n",
    "                step_rewards += optrewards\n",
    "                step += optsteps\n",
    "\n",
    "\n",
    "        episode_rewards.append(step_rewards)\n",
    "        episode_updates.append(step_updates)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Run: {run+1}/{max_runs}, Episode: {episode+1}/{max_episodes}: {step_rewards}\")\n",
    "\n",
    "    run_rewards.append(episode_rewards)\n",
    "    updates_Intra.append(episode_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
