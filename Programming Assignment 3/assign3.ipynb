{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.11 install gym==0.15.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space details: Discrete(500)\n",
      "Action space details: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# Setting up the Taxi-v3 scenario\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "\n",
    "# Displaying the state space information\n",
    "print(\"State space details:\", env.observation_space)\n",
    "\n",
    "# Displaying the action space information\n",
    "print(\"Action space details:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the taxi (Row, Column): (4, 1)\n",
      "Location of the passenger: 0\n",
      "Destination of the passenger: 1\n"
     ]
    }
   ],
   "source": [
    "# Retrieving details from the environment state\n",
    "taxi_row, taxi_column, passenger_index, destination_index = list(env.decode(env.s))\n",
    "print(\"Current position of the taxi (Row, Column):\", (taxi_row, taxi_column))\n",
    "print(\"Location of the passenger:\", passenger_index)\n",
    "print(\"Destination of the passenger:\", destination_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action selected: 5\n",
      "Next state after action: 421\n",
      "Reward received: -10\n",
      "Is the new state terminal? False\n",
      "Transition probability: {'prob': 1.0}\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "# Randomly choosing an action from a range of 6 actions\n",
    "random_action = np.random.choice(np.arange(6))\n",
    "print(\"Action selected:\", random_action)\n",
    "\n",
    "# Taking a step in the environment based on the chosen action\n",
    "new_state, earned_reward, is_done, transition_prob = env.step(random_action)\n",
    "print(\"Next state after action:\", new_state)\n",
    "print(\"Reward received:\", earned_reward)\n",
    "print(\"Is the new state terminal?\", is_done)\n",
    "print(\"Transition probability:\", transition_prob)\n",
    "\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to Location \"R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_R(state, environment):\n",
    "\n",
    "    # Decode the state to extract row and column details\n",
    "    agent_row, agent_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination check\n",
    "    is_goal_reached = False\n",
    "    selected_move = 0  # Default move\n",
    "\n",
    "    # Check if the agent has reached the goal position\n",
    "    if (agent_row == 0 and agent_col == 0):\n",
    "        is_goal_reached = True\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move upwards if the agent is in column 0\n",
    "    if (agent_col == 0):\n",
    "        selected_move = 1  # Move up\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Move downwards if at (0, 2) or (1, 2)\n",
    "    if ((agent_row == 0 and agent_col == 2) or (agent_row == 1 and agent_col == 2)):\n",
    "        selected_move = 0  # Move down\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move upwards if at (3, 1), (3, 3), (4, 1), or (4, 3)\n",
    "    if ((agent_row == 3 and agent_col == 1) or (agent_row == 3 and agent_col == 3) or \n",
    "        (agent_row == 4 and agent_col == 1) or (agent_row == 4 and agent_col == 3)):\n",
    "        selected_move = 1  # Move up\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Move left otherwise\n",
    "    selected_move = 3  # Move left\n",
    "    return is_goal_reached, selected_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "_, selected_move = navigate_to_R(env.s, env)\n",
    "env.step(selected_move)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to Location \"Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_Y(state, environment):\n",
    "\n",
    "    # Decode the state to extract row and column information\n",
    "    agent_row, agent_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_goal_reached = False\n",
    "    selected_move = 0  # Default move\n",
    "\n",
    "    # Check if the agent has reached the target position\n",
    "    if (agent_row == 4 and agent_col == 0):\n",
    "        is_goal_reached = True\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move down if the agent is in column 0\n",
    "    if (agent_col == 0):\n",
    "        selected_move = 0  # Move down\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Move down if at (0, 2) or (1, 2)\n",
    "    if ((agent_row == 0 and agent_col == 2) or (agent_row == 1 and agent_col == 2)):\n",
    "        selected_move = 0  # Move down\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move up if at (3, 1), (3, 3), (4, 1), or (4, 3)\n",
    "    if ((agent_row == 3 and agent_col == 1) or (agent_row == 3 and agent_col == 3) or \n",
    "        (agent_row == 4 and agent_col == 1) or (agent_row == 4 and agent_col == 3)):\n",
    "        selected_move = 1  # Move up\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Otherwise, move left\n",
    "    selected_move = 3  # Move left\n",
    "    return is_goal_reached, selected_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "_, selected_move = navigate_to_Y(env.s, env)\n",
    "env.step(selected_move)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to Location \"G\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_G(state, environment):\n",
    "\n",
    "    # Decode the state to extract row and column information\n",
    "    agent_row, agent_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_goal_reached = False\n",
    "    selected_move = 0  # Default move\n",
    "\n",
    "    # Check if the agent has reached the goal position\n",
    "    if (agent_row == 0 and agent_col == 4):\n",
    "        is_goal_reached = True\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move up if the agent is in column 4\n",
    "    if (agent_col == 4):\n",
    "        selected_move = 1  # Move up\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Move down if at (0, 1) or (1, 1)\n",
    "    if ((agent_row == 0 and agent_col == 1) or (agent_row == 1 and agent_col == 1)):\n",
    "        selected_move = 0  # Move down\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move up if at (3, 0), (3, 2), (4, 0), or (4, 2)\n",
    "    if ((agent_row == 3 and agent_col == 0) or (agent_row == 3 and agent_col == 2) or \n",
    "        (agent_row == 4 and agent_col == 0) or (agent_row == 4 and agent_col == 2)):\n",
    "        selected_move = 1  # Move up\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Otherwise, move right\n",
    "    selected_move = 2  # Move right\n",
    "    return is_goal_reached, selected_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "_, selected_move = navigate_to_G(env.s, env)\n",
    "env.step(selected_move)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to Location \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_B(state, environment):\n",
    "\n",
    "    # Decode the state to extract row and column information\n",
    "    agent_row, agent_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_goal_reached = False\n",
    "    selected_move = 0  # Default move\n",
    "\n",
    "    # Check if the agent has reached the destination\n",
    "    if (agent_row == 4 and agent_col == 3):\n",
    "        is_goal_reached = True\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Move left if at (4, 4)\n",
    "    if (agent_row == 4 and agent_col == 4):\n",
    "        selected_move = 3  # Move left\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move down if in column 3 or 4\n",
    "    if (agent_col == 3 or agent_col == 4):\n",
    "        selected_move = 0  # Move down\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Move down if at (0, 1) or (1, 1)\n",
    "    if ((agent_row == 0 and agent_col == 1) or (agent_row == 1 and agent_col == 1)):\n",
    "        selected_move = 0  # Move down\n",
    "        return is_goal_reached, selected_move\n",
    "  \n",
    "    # Move up if at (3, 0), (3, 2), (4, 0), or (4, 2)\n",
    "    if ((agent_row == 3 and agent_col == 0) or (agent_row == 3 and agent_col == 2) or \n",
    "        (agent_row == 4 and agent_col == 0) or (agent_row == 4 and agent_col == 2)):\n",
    "        selected_move = 1  # Move up\n",
    "        return is_goal_reached, selected_move\n",
    "\n",
    "    # Otherwise, move right\n",
    "    selected_move = 2  # Move right\n",
    "    return is_goal_reached, selected_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "_, selected_move = navigate_to_B(env.s, env)\n",
    "env.step(selected_move)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up initializations and auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_funcs = [navigate_to_R, navigate_to_Y, navigate_to_G, navigate_to_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(q_values, current_state, actions_available, epsilon_value):\n",
    "    state_action_values = q_values[current_state, np.array(actions_available)]\n",
    "    if ( (np.random.rand() < epsilon_value) or (not state_action_values.any()) ): \n",
    "        return np.random.choice(actions_available)\n",
    "    else:\n",
    "        return actions_available[np.argmax(state_action_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_available_options(state, environment):\n",
    "\n",
    "    agent_row, agent_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    available_actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "    if (agent_row == 0 and agent_col == 0):\n",
    "        available_actions.remove(6)\n",
    "        return available_actions\n",
    "\n",
    "    if (agent_row == 4 and agent_col == 0):\n",
    "        available_actions.remove(7)\n",
    "        return available_actions\n",
    "\n",
    "    if (agent_row == 0 and agent_col == 4):\n",
    "        available_actions.remove(8)\n",
    "        return available_actions\n",
    "\n",
    "    if (agent_row == 4 and agent_col == 3):\n",
    "        available_actions.remove(9)\n",
    "        return available_actions\n",
    "\n",
    "    return available_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMDP Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMDPTrainer:\n",
    "    \"\"\"\n",
    "    Helper class for SMDP Q-Learning training and visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma_rate=0.9, learn_rate=0.1, exploration_rate=0.1, option_functions=None, available_options_fn=None):\n",
    "        self.gamma_rate = gamma_rate\n",
    "        self.learn_rate = learn_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exp_name = f'gamma_{int(self.gamma_rate*100)}_learn_{int(self.learn_rate*1000)}_exploration_{int(self.exploration_rate*1000)}'\n",
    "        self.q_values = np.zeros((500, 10))\n",
    "        self.update_freq = np.zeros((500, 10))\n",
    "        self.option_functions = option_functions\n",
    "        self.available_options_fn = available_options_fn\n",
    "        self.env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "    def train(self, num_episodes=3000, verbose=True):\n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_rewards = np.zeros(num_episodes)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        for episode in (tqdm(range(num_episodes)) if verbose else range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                available_actions = self.available_options_fn(state, self.env)\n",
    "                action = self.egreedy_action(self.q_values, state, available_actions)\n",
    "\n",
    "                if action < 6:\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    self.q_values[state, action] += self.learn_rate * (\n",
    "                        reward + self.gamma_rate * np.max(self.q_values[next_state, :]) - self.q_values[state, action]\n",
    "                    )\n",
    "                    self.update_freq[state, action] += 1\n",
    "                    state = next_state\n",
    "                    episode_reward += reward\n",
    "\n",
    "                if action >= 6:\n",
    "                    reward_accumulator = 0\n",
    "                    option_done = False\n",
    "                    starting_state = state\n",
    "                    time_steps = 0\n",
    "                    opt_fn = self.option_functions[action-6]\n",
    "\n",
    "                    while not option_done:\n",
    "                        option_done, option_action = opt_fn(state, self.env)\n",
    "\n",
    "                        if option_done:\n",
    "                            self.q_values[starting_state, action] += self.learn_rate * (\n",
    "                                reward_accumulator + (self.gamma_rate**time_steps) * np.max(self.q_values[state, :]) - self.q_values[starting_state, action]\n",
    "                            )\n",
    "                            self.update_freq[starting_state, action] += 1\n",
    "                            break\n",
    "\n",
    "                        next_state, reward, done, _ = self.env.step(option_action)\n",
    "                        time_steps += 1      \n",
    "                        reward_accumulator += (self.gamma_rate**(time_steps - 1)) * reward      \n",
    "                        episode_reward += reward\n",
    "                        state = next_state\n",
    "\n",
    "            self.episode_rewards[episode] = episode_reward\n",
    "\n",
    "        return self.episode_rewards, self.q_values, self.update_freq\n",
    "\n",
    "    def plot_rewards(self, save=False):\n",
    "        avg_100_reward = np.array([np.mean(self.episode_rewards[max(0, i-100):i]) for i in range(1, len(self.episode_rewards)+1)])\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Episode Reward')\n",
    "        plt.title('Rewards vs Episodes: Avg Reward: %.3f' % np.mean(self.episode_rewards))\n",
    "        plt.plot(np.arange(self.num_episodes), self.episode_rewards, 'b')\n",
    "        plt.plot(np.arange(self.num_episodes), avg_100_reward, 'r', linewidth=1.5)\n",
    "        if save: \n",
    "            plt.savefig(f'./smdp/{self.exp_name}_rewards.jpg', pad_inches=0)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_updates(self, save=False):\n",
    "        total_updates = np.sum(self.update_freq, axis=1)\n",
    "        grid_updates = np.zeros((5, 5))\n",
    "\n",
    "        for state in range(500):\n",
    "            row, col, _, _ = self.env.decode(state)\n",
    "            grid_updates[row, col] += total_updates[state]\n",
    "\n",
    "        sns.heatmap(grid_updates, annot=True, fmt='g', square=True, cmap='viridis')\n",
    "        plt.title('Update Frequency Table for SMDP Q-Learning')\n",
    "        if save: \n",
    "            plt.savefig(f'./smdp/{self.exp_name}_updates.jpg', pad_inches=0)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_q_values(self, save=False):\n",
    "        q_values_pickup = np.zeros((4, 5, 5, 10))\n",
    "        q_values_drop = np.zeros((4, 5, 5, 10))\n",
    "\n",
    "        for state in range(500):\n",
    "            row, col, src, dest = self.env.decode(state)\n",
    "            if src < 4 and src != dest:\n",
    "                q_values_pickup[src][row][col] += self.q_values[state]\n",
    "            if src == 4:\n",
    "                q_values_drop[dest][row][col] += self.q_values[state]\n",
    "\n",
    "        for phase, q_values in zip(['Pick', 'Drop'], [q_values_pickup, q_values_drop]):\n",
    "            for pos in ['R', 'G', 'Y', 'B']:\n",
    "                sns.heatmap(q_values[pos], annot=True, square=True, cbar=False, \n",
    "                            cbar_kws={'ticks': range(10)}, vmin=0, vmax=9, cmap='viridis')\n",
    "                plt.title(f'Q-Values for SMDP Q-Learning: {phase} at {pos}')\n",
    "                if save: \n",
    "                    plt.savefig(f'./smdp/{self.exp_name}_q_vals_{phase}_{pos}.jpg', pad_inches=0)\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hyperparameters(file_path):\n",
    "    with open(file_path, 'w') as log_file:\n",
    "        sys.stdout = log_file\n",
    "        \n",
    "        alphas = [0.5, 0.1, 0.05, 0.01]\n",
    "        gammas = [0.90]\n",
    "        epsilons = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "\n",
    "        best_reward = -np.inf\n",
    "        best_params = {'alpha': None, 'gamma': None, 'epsilon': None}\n",
    "\n",
    "        config_count = 1\n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for epsilon in epsilons:\n",
    "                    print(\"Testing Configuration:\", config_count)\n",
    "                    print('Hyperparameters: [alpha = {}, gamma = {}, epsilon = {}]'.format(alpha, gamma, epsilon))\n",
    "                    \n",
    "                    agent = SMDPTrainer(alpha=alpha, epsilon=epsilon, gamma_rate=gamma)\n",
    "                    rewards, _, _ = agent.train(verbose=False)\n",
    "                    avg_reward = np.mean(rewards)\n",
    "                    print('Average Reward:', avg_reward)\n",
    "                    print('***************************************************************************\\n')\n",
    "\n",
    "                    if avg_reward > best_reward:\n",
    "                        best_reward = avg_reward\n",
    "                        best_params['alpha'] = alpha\n",
    "                        best_params['gamma'] = gamma\n",
    "                        best_params['epsilon'] = epsilon\n",
    "\n",
    "                    config_count += 1\n",
    "\n",
    "        print('\\nBest Reward:', best_reward)\n",
    "        print('Best Hyperparameters:', best_params)\n",
    "\n",
    "test_hyperparameters('./logger/smdp_log1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for plotting\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Initialize an empty list to store rewards\n",
    "all_rewards = []\n",
    "\n",
    "# Run training for 10 iterations\n",
    "for _ in range(10):\n",
    "    agent = SMDPTrainer(alpha_value=0.5, epsilon_value=0.1, gamma_rate=0.9)\n",
    "    episode_rewards, _, _ = agent.train(verbose=True)\n",
    "    all_rewards.append(episode_rewards)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "all_rewards = np.array(all_rewards)\n",
    "\n",
    "# Calculate the average rewards across all iterations\n",
    "average_rewards = np.mean(all_rewards, axis=0)\n",
    "\n",
    "# Calculate the average reward over a rolling window of 100 episodes\n",
    "rolling_avg_rewards = np.array([np.mean(average_rewards[max(0, i-100):i]) for i in range(1, len(average_rewards) + 1)])\n",
    "\n",
    "# Plotting the rewards\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.title(f'Rewards vs Episodes: Avg Reward: {np.mean(average_rewards):.3f}')\n",
    "plt.plot(np.arange(3000), average_rewards, 'b')\n",
    "plt.plot(np.arange(3000), rolling_avg_rewards, 'r', linewidth=1.5)\n",
    "plt.savefig('./smdp/rewards.jpg', pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_zero = SMDPTrainer(alpha_value=0.5, epsilon_value=0.1, gamma_rate=0.9)\n",
    "\n",
    "# Train the agent and retrieve rewards, Q-values, and update frequencies\n",
    "rewards_zero, q_values_zero, update_frequencies_zero = agent_zero.train(verbose=True)\n",
    "\n",
    "# Plot the reward curve and save the plot\n",
    "agent_zero.plot_rewards(save=True)\n",
    "\n",
    "# Plot the update frequency and save the plot\n",
    "agent_zero.plot_update_frequency(save=True)\n",
    "\n",
    "# Plot the Q-values and save the plot\n",
    "agent_zero.plot_q_values(save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Option Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntraOption:\n",
    "    \"\"\"\n",
    "    Custom Intra Option\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_val=0.9, alpha_val=0.1, epsilon_val=0.1, opt_functions=option_funcs, gen_available_options=gen_available_options):\n",
    "        self.gamma = gamma_val\n",
    "        self.alpha = alpha_val\n",
    "        self.epsilon = epsilon_val\n",
    "        self.experiment_name = 'a' + str(int(self.alpha * 1000)) + '_e' + str(int(self.epsilon * 1000)) + '_g' + str(int(self.gamma * 100))\n",
    "        self.q_values = np.zeros((500, 10))\n",
    "        self.update_frequency = np.zeros((500, 10))\n",
    "        self.gen_available_options = gen_available_options\n",
    "        self.opt_functions = opt_functions\n",
    "        \n",
    "        self.environment = gym.make(\"Taxi-v3\")\n",
    "        \n",
    "    def training(self, num_episodes=3000, is_verbose=True):\n",
    "        \n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_rewards = np.zeros(num_episodes)\n",
    "        self.is_verbose = is_verbose\n",
    "        \n",
    "        for episode in (tqdm(range(num_episodes)) if is_verbose else range(num_episodes)):\n",
    "\n",
    "            state = self.environment.reset()    \n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                available_actions = self.gen_available_options(state, self.environment)\n",
    "\n",
    "                action = epsilon_greedy_policy(self.q_values, state, available_actions, self.epsilon)\n",
    "\n",
    "                if action < 6:\n",
    "                    next_state, reward, done, _ = self.environment.step(action)\n",
    "                    self.q_values[state, action] += self.alpha * (reward + self.gamma * np.max(self.q_values[next_state, :]) - self.q_values[state, action])\n",
    "                    self.update_frequency[state, action] += 1\n",
    "\n",
    "                    for j in range(4):\n",
    "                        opt_fn = self.opt_functions[j]\n",
    "                        opt_id = j + 6\n",
    "                        opt_done, opt_action = opt_fn(state, self.environment)\n",
    "\n",
    "                        if opt_action == action:\n",
    "                            self.q_values[state, opt_id] += self.alpha * (reward + self.gamma * np.max(self.q_values[next_state, :]) - self.q_values[state, opt_id])\n",
    "                            self.update_frequency[state, opt_id] += 1\n",
    "\n",
    "                    state = next_state\n",
    "                    episode_reward += reward\n",
    "\n",
    "                elif action >= 6: \n",
    "                    opt_done = False\n",
    "                    start_state = state\n",
    "                    \n",
    "                    opt_fn = self.opt_functions[action - 6]\n",
    "                    time_steps = 0\n",
    "\n",
    "                    while not opt_done:\n",
    "                        opt_done, opt_action = opt_fn(state, self.environment)\n",
    "                        next_state, reward, done, _ = self.environment.step(opt_action)\n",
    "                        start_state = state   \n",
    "                        state = next_state\n",
    "\n",
    "                        time_steps += 1\n",
    "                        episode_reward += reward * (self.gamma ** (time_steps - 1))\n",
    "\n",
    "                        if opt_done:\n",
    "                            self.q_values[start_state, action] += self.alpha * (reward + (self.gamma) * np.max(self.q_values[state, :]) - self.q_values[start_state, action])\n",
    "                            self.update_frequency[start_state, action] += 1\n",
    "\n",
    "                            self.q_values[start_state, opt_action] += self.alpha * (reward + (self.gamma) * np.max(self.q_values[state, :]) - self.q_values[start_state, opt_action])\n",
    "                            self.update_frequency[start_state, opt_action] += 1\n",
    "\n",
    "                            for j in range(4):\n",
    "                                opt2_fn = self.opt_functions[j]\n",
    "                                opt2_id = j + 6\n",
    "                                opt2_done, opt2_action = opt_fn(state, self.environment)\n",
    "\n",
    "                                if opt_action == opt2_action:\n",
    "                                    self.q_values[start_state, opt2_id] += self.alpha * (reward + self.gamma * np.max(self.q_values[state, :]) - self.q_values[start_state, opt2_id])\n",
    "                                    self.update_frequency[state, opt2_id] += 1\n",
    "\n",
    "                        self.q_values[start_state, action] += self.alpha * (reward + (self.gamma) * (self.q_values[state, action]) - self.q_values[start_state, action])\n",
    "                        self.update_frequency[start_state, action] += 1\n",
    "\n",
    "                        self.q_values[start_state, opt_action] += self.alpha * (reward + (self.gamma) * (self.q_values[state, opt_action]) - self.q_values[start_state, opt_action])\n",
    "                        self.update_frequency[start_state, opt_action] += 1\n",
    "\n",
    "                        for j in range(4):\n",
    "                            opt2_fn = self.opt_functions[j]\n",
    "                            opt2_id = j + 6\n",
    "                            opt2_done, opt2_action = opt_fn(state, self.environment)\n",
    "\n",
    "                            if opt_action == opt2_action:\n",
    "                                self.q_values[start_state, opt2_id] += self.alpha * (reward + self.gamma * (self.q_values[state, opt2_id]) - self.q_values[start_state, opt2_id])\n",
    "                                self.update_frequency[state, opt2_id] += 1\n",
    "\n",
    "            self.episode_rewards[episode] = episode_reward\n",
    "        return self.episode_rewards, self.q_values, self.update_frequency\n",
    "            \n",
    "    def plot_rewards(self, save=False):\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        avg_100_reward = np.array([np.mean(self.episode_rewards[max(0, i - 100):i]) for i in range(1, len(self.episode_rewards) + 1)])\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Episode Reward')\n",
    "        plt.title('Rewards vs Episodes: Avg Reward: %.3f' % np.mean(self.episode_rewards))\n",
    "        plt.plot(np.arange(self.num_episodes), self.episode_rewards, 'b')\n",
    "        plt.plot(np.arange(self.num_episodes), avg_100_reward, 'r', linewidth=1.5)\n",
    "        if save: \n",
    "            plt.savefig('./intraop/' + self.experiment_name + '_rewards.jpg', pad_inches=0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_update_frequency(self, save=False):\n",
    "        total_updates = np.sum(self.update_frequency, axis=1)\n",
    "        grid_updates = np.zeros((5, 5))\n",
    "        \n",
    "        for state in range(500):\n",
    "            row, col, src, dst = self.environment.decode(state)\n",
    "            grid_updates[row, col] += total_updates[state]\n",
    "            \n",
    "        sns.heatmap(grid_updates, annot=True, fmt='g', square=True, cmap='viridis')\n",
    "        plt.title('Update Frequency Table for Intra-Option Q-Learning')\n",
    "        if save: \n",
    "            plt.savefig('./intraop/' + self.experiment_name + '_updates.jpg', pad_inches=0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_q_values(self, save=False):\n",
    "        pickup_q_values = np.zeros((4, 5, 5, 10))\n",
    "        q_values = np.zeros((2, 4, 5, 5))\n",
    "        for state in range(500):\n",
    "            row, col, src, dest = self.environment.decode(state)\n",
    "            if src < 4 and src != dest:\n",
    "                pickup_q_values[src][row][col] += self.q_values[state]\n",
    "        \n",
    "        for state in range(500):\n",
    "            row, col, src, dest = self.environment.decode(state)\n",
    "            if src < 4 and src != dest:\n",
    "                q_values[0][src][row][col] = np.argmax(pickup_q_values[src][row][col])\n",
    "            if src == 4:\n",
    "                q_values[1][dest][row][col] = np.argmax(self.q_values[state])\n",
    "        \n",
    "        phase = ['Pick', 'Drop']\n",
    "        positions = ['R', 'G', 'Y', 'B']\n",
    "        for i in range(2):\n",
    "            for j in range(4):\n",
    "                sns.heatmap(q_values[i][j], annot=True, square=True, cbar=False, cbar_kws={'ticks': range(10)}, vmin=0, vmax=9, cmap='viridis')\n",
    "                plt.title('Q-Values for Intra-Option Q-Learning: {} at {}'.format(phase[i], positions[j]))\n",
    "                if save: \n",
    "                    plt.savefig('./intraop/' + self.experiment_name + '_q_vals_' + phase[i] + '_' + positions[j] + '.jpg', pad_inches=0)\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hyperparameters(alphas, gammas, epsilons):\n",
    "    best_reward = -np.inf\n",
    "    best_hyperparams = {'alpha': None, 'gamma': None, 'epsilon': None}\n",
    "\n",
    "    config_count = 1\n",
    "\n",
    "    with open('./logs/intraop_log1.txt', 'w') as f:\n",
    "        sys.stdout = f  # Redirect stdout to the log file\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for epsilon in epsilons:\n",
    "                    print(\"Testing Configuration:\", config_count)\n",
    "                    print('Hyperparameters: [alpha = {}, gamma = {}, epsilon = {}]'.format(alpha, gamma, epsilon))\n",
    "                    \n",
    "                    agent = IntraOption(alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "                    \n",
    "                    rewards, q_values, update_freq = agent.training(verbose=False)\n",
    "                    avg_reward = np.mean(rewards)\n",
    "                    \n",
    "                    print('Average Reward:', avg_reward)\n",
    "                    print('***************************************************************************\\n')\n",
    "\n",
    "                    if avg_reward > best_reward:\n",
    "                        best_reward = avg_reward\n",
    "                        best_hyperparams['alpha'] = alpha\n",
    "                        best_hyperparams['gamma'] = gamma\n",
    "                        best_hyperparams['epsilon'] = epsilon\n",
    "\n",
    "                    config_count += 1\n",
    "\n",
    "        print('\\nBest Reward:', best_reward)\n",
    "        print('Best Hyperparameters:', best_hyperparams)\n",
    "\n",
    "# Define hyperparameters\n",
    "alphas = [0.5, 0.1, 0.05, 0.01]\n",
    "gammas = [0.90]\n",
    "epsilons = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "\n",
    "# Test hyperparameters\n",
    "test_hyperparameters(alphas, gammas, epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "\n",
    "def test_rewards():\n",
    "    reward_list = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        agent = IntraOption(alpha=0.5, epsilon=0.001, gamma=0.9)\n",
    "        rewards, _, _= agent.trainer(verbose=True)\n",
    "        reward_list.append(rewards)\n",
    "    \n",
    "    reward_list = np.array(reward_list)\n",
    "    mean_rewards = np.mean(reward_list, axis=0)\n",
    "    avg_100_rewards = np.array([np.mean(mean_rewards[max(0,i-100):i]) for i in range(1, len(mean_rewards)+1)])\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Episode Reward')\n",
    "    plt.title('Rewards vs Episodes: Avg Reward: %.3f' % np.mean(mean_rewards))\n",
    "    plt.plot(np.arange(3000), mean_rewards, 'b')\n",
    "    plt.plot(np.arange(3000), avg_100_rewards, 'r', linewidth=1.5)\n",
    "    plt.savefig('./intraop/rewards.jpg', pad_inches=0)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to test rewards\n",
    "test_rewards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_one = IntraOption(alpha_value=0.5, epsilon_value=0.001, gamma_rate=0.9)\n",
    "\n",
    "# Train the agent and retrieve rewards, Q-values, and update frequencies\n",
    "rewards_one, q_values_one, update_frequencies_one = agent_one.train(verbose=True)\n",
    "\n",
    "# Plot the reward curve and save the plot\n",
    "agent_one.plot_rewards(save=True)\n",
    "\n",
    "# Plot the update frequency and save the plot\n",
    "agent_one.plot_update_frequency(save=True)\n",
    "\n",
    "# Plot the Q-values and save the plot\n",
    "agent_one.plot_q_values(save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMDP vs IntraOption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('darkgrid')\n",
    "\n",
    "avg10_reward_smdp = np.array([np.mean(rewards_zero[max(0, i-10):i]) for i in range(1, len(rewards_zero)+1)])\n",
    "avg10_reward_intraop = np.array([np.mean(rewards_one[max(0, i-10):i]) for i in range(1, len(rewards_one)+1)])\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.title('Rewards vs Episodes: SMDP & IntraOp')\n",
    "plt.plot(np.arange(len(rewards_zero)), avg10_reward_smdp)\n",
    "plt.plot(np.arange(len(rewards_one)), avg10_reward_intraop)\n",
    "plt.legend(['SMDP', 'IntraOp'])\n",
    "plt.savefig('./gen_imgs/smdp_vs_intraop_rewards.jpg', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Calculate total updates for SMDP and IntraOp\n",
    "total_updates_smdp = np.sum(update_frequencies_zero, axis=1)\n",
    "total_updates_intraop = np.sum(update_frequencies_one, axis=1)\n",
    "\n",
    "# Initialize grid updates matrices\n",
    "grid_updates_smdp = np.zeros((5, 5))\n",
    "grid_updates_intraop = np.zeros((5, 5))\n",
    "\n",
    "# Update grid updates matrices\n",
    "for state in range(500):\n",
    "    row, col, src, dst = env.decode(state)\n",
    "    grid_updates_smdp[row, col] += total_updates_smdp[state]\n",
    "    grid_updates_intraop[row, col] += total_updates_intraop[state]\n",
    "\n",
    "# Determine color scale limits\n",
    "vmin = min(np.min(grid_updates_smdp), np.min(grid_updates_intraop))\n",
    "vmax = max(np.max(grid_updates_smdp), np.max(grid_updates_intraop))\n",
    "\n",
    "# Create subplots for SMDP and IntraOp update frequency comparison\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Update Frequency Table: SMDP vs IntraOp')\n",
    "\n",
    "# Plot heatmaps for SMDP and IntraOp\n",
    "sns.heatmap(grid_updates_smdp, annot=True, fmt='g', square=True, cmap='viridis', cbar=False, ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "sns.heatmap(grid_updates_intraop, annot=True, fmt='g', square=True, cmap='viridis', cbar=False, ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "\n",
    "# Set figure size and save the plot\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(4.5)\n",
    "plt.savefig('./gen_imgs/smdp_vs_intraop_updates.jpg', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Alternate Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_south(state, environment):\n",
    "    # Decode the state information\n",
    "    current_row, current_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_done = False\n",
    "    selected_action = 0\n",
    "\n",
    "    # Check if the target row (South direction) is reached\n",
    "    if current_row == 4:\n",
    "        is_done = True\n",
    "        return is_done, selected_action\n",
    "\n",
    "    # Move in the South direction\n",
    "    selected_action = 0  # Represents the action for moving South\n",
    "    return is_done, selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_north(state, environment):\n",
    "    # Decode the state information\n",
    "    current_row, current_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_done = False\n",
    "    selected_action = 0\n",
    "\n",
    "    # Check if the target row (North direction) is reached\n",
    "    if current_row == 0:\n",
    "        is_done = True\n",
    "        return is_done, selected_action\n",
    "\n",
    "    # Move in the North direction\n",
    "    selected_action = 1  # Represents the action for moving North\n",
    "    return is_done, selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_east(state, environment):\n",
    "\n",
    "    # Decode the state information\n",
    "    current_row, current_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_done = False\n",
    "    selected_action = 0\n",
    "\n",
    "    # Check if the target (East direction) is reached\n",
    "    if (\n",
    "        current_col == 4\n",
    "        or (\n",
    "            (current_row == 0 and current_col == 1)\n",
    "            or (current_row == 1 and current_col == 1)\n",
    "            or (current_row == 3 and current_col == 0)\n",
    "            or (current_row == 3 and current_col == 2)\n",
    "            or (current_row == 4 and current_col == 0)\n",
    "            or (current_row == 4 and current_col == 2)\n",
    "        )\n",
    "    ):\n",
    "        is_done = True\n",
    "        return is_done, selected_action\n",
    "\n",
    "    # Move in the East direction\n",
    "    selected_action = 2  # Represents the action for moving East\n",
    "    return is_done, selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_west(state, environment):\n",
    "\n",
    "    # Decode the state information\n",
    "    current_row, current_col, _, _ = list(environment.decode(state))\n",
    "\n",
    "    # Initialize default values for action and termination condition\n",
    "    is_done = False\n",
    "    selected_action = 0\n",
    "\n",
    "    # Check if the target (West direction) is reached\n",
    "    if (\n",
    "        current_col == 0\n",
    "        or (\n",
    "            (current_row == 0 and current_col == 2)\n",
    "            or (current_row == 1 and current_col == 2)\n",
    "            or (current_row == 3 and current_col == 1)\n",
    "            or (current_row == 3 and current_col == 3)\n",
    "            or (current_row == 4 and current_col == 1)\n",
    "            or (current_row == 4 and current_col == 3)\n",
    "        )\n",
    "    ):\n",
    "        is_done = True\n",
    "        return is_done, selected_action\n",
    "\n",
    "    # Move in the West direction\n",
    "    selected_action = 3  # Represents the action for moving West\n",
    "    return is_done, selected_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_available_options(state, environment):\n",
    "\n",
    "    # Decode the state information\n",
    "    current_row, current_col, _, _ = list(environment.decode(state))\n",
    "    \n",
    "    # Initialize the list of available actions\n",
    "    available_actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "    # Check for conditions and remove unavailable actions accordingly\n",
    "    if current_row == 4:\n",
    "        available_actions.pop(6)\n",
    "        return available_actions\n",
    "\n",
    "    if current_row == 0:\n",
    "        available_actions.pop(7)\n",
    "        return available_actions\n",
    "\n",
    "    if (\n",
    "        (current_col == 4)\n",
    "        or ((current_row == 0 and current_col == 1) or (current_row == 1 and current_col == 1))\n",
    "        or ((current_row == 3 and current_col == 0) or (current_row == 3 and current_col == 2) or (current_row == 4 and current_col == 0) or (current_row == 4 and current_col == 2))\n",
    "    ):\n",
    "        available_actions.pop(8)\n",
    "        return available_actions\n",
    "\n",
    "    if (\n",
    "        (current_col == 0)\n",
    "        or ((current_row == 0 and current_col == 2) or (current_row == 1 and current_col == 2))\n",
    "        or ((current_row == 3 and current_col == 1) or (current_row == 3 and current_col == 3) or (current_row == 4 and current_col == 1) or (current_row == 4 and current_col == 3))\n",
    "    ):\n",
    "        available_actions.pop(9)\n",
    "        return available_actions\n",
    "\n",
    "    return available_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_funcs_new = [move_south, move_north, move_east, move_west]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra-Option Q-Learning with Alternate Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hyperparameters(logfile_path='./logs/intraop_log2.txt'):\n",
    "    with open(logfile_path, 'w') as logfile:\n",
    "        sys.stdout = logfile\n",
    "\n",
    "        learning_rates = [0.5, 0.1, 0.05, 0.01]\n",
    "        discount_factors = [0.90]\n",
    "        exploration_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "        best_reward = -float('inf')\n",
    "        best_hyperparameters = {'alpha': None, 'gamma': None, 'epsilon': None}\n",
    "\n",
    "        config_number = 1\n",
    "        for alpha in learning_rates:\n",
    "            for gamma in discount_factors:\n",
    "                for epsilon in exploration_rates:\n",
    "                    print(\"Testing Configuration:\", config_number)\n",
    "                    print(f'Hyperparameters: [alpha = {alpha}, gamma = {gamma}, epsilon = {epsilon}]')\n",
    "\n",
    "                    agent = IntraOption(alpha=alpha, epsilon=epsilon, gamma=gamma, opt_fns=options_funcs_new, gen_avl_options=generate_available_options)\n",
    "                    rewards, _, _ = agent.trainer(verbose=False)\n",
    "                    avg_reward = np.mean(rewards)\n",
    "                    print('Average Reward:', avg_reward)\n",
    "                    print('***************************************************************************\\n')\n",
    "\n",
    "                    if avg_reward > best_reward:\n",
    "                        best_reward = avg_reward\n",
    "                        best_hyperparameters['alpha'] = alpha\n",
    "                        best_hyperparameters['gamma'] = gamma\n",
    "                        best_hyperparameters['epsilon'] = epsilon\n",
    "\n",
    "                    config_number += 1\n",
    "\n",
    "        print('\\nBest Reward:', best_reward)\n",
    "        print('Best Hyperparameters:', best_hyperparameters)\n",
    "\n",
    "# Call the function to test hyperparameters\n",
    "test_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plot style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Initialize an empty list to store rewards\n",
    "reward_list = []\n",
    "\n",
    "# Run the training loop 10 times\n",
    "for _ in range(10):\n",
    "    # Create an IntraOption agent with specified hyperparameters\n",
    "    agent = IntraOption(alpha=0.5, epsilon=0.001, gamma=0.9, opt_fns=options_funcs_new, gen_avl_options=generate_available_options)\n",
    "    \n",
    "    # Train the agent and collect rewards\n",
    "    rewards, _, _ = agent.trainer(verbose=True)\n",
    "    reward_list.append(rewards)\n",
    "\n",
    "# Convert the reward list to a numpy array\n",
    "reward_array = np.array(reward_list)\n",
    "\n",
    "# Calculate the mean rewards across episodes\n",
    "mean_rewards = np.mean(reward_array, axis=0)\n",
    "\n",
    "# Calculate the rolling average over 100 episodes\n",
    "rolling_avg_rewards = np.array([np.mean(mean_rewards[max(0, i - 100):i]) for i in range(1, len(mean_rewards) + 1)])\n",
    "\n",
    "# Plotting\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.title('Rewards vs Episodes: Avg Reward: {:.3f}'.format(np.mean(mean_rewards)))\n",
    "plt.plot(np.arange(3000), mean_rewards, 'b')\n",
    "plt.plot(np.arange(3000), rolling_avg_rewards, 'r', linewidth=1.5)\n",
    "plt.savefig('./intraop/rewards_new.jpg', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_11 = IntraOption(alpha_value=0.5, epsilon_value=0.001, gamma_rate=0.9, opt_fns=options_funcs_new, gen_avl_options=generate_available_options)\n",
    "\n",
    "# Train the agent and retrieve rewards, Q-values, and update frequencies\n",
    "rewards_11, q_values_11, update_frequencies_11 = agent_11.train(verbose=True)\n",
    "\n",
    "# Plot the reward curve and save the plot\n",
    "agent_11.plot_rewards(save=True)\n",
    "\n",
    "# Plot the update frequency and save the plot\n",
    "agent_11.plot_update_frequency(save=True)\n",
    "\n",
    "# Plot the Q-values and save the plot\n",
    "agent_11.plot_q_values(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Calculate the rolling average over 10 episodes for old and new options\n",
    "avg_10_reward1 = np.array([np.mean(rewards_one[max(0, i - 10):i]) for i in range(1, len(rewards_one) + 1)])\n",
    "avg_10_reward11 = np.array([np.mean(rewards_11[max(0, i - 10):i]) for i in range(1, len(rewards_11) + 1)])\n",
    "\n",
    "# Plotting\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.title('Rewards for IntraOp - Old vs New Options')\n",
    "plt.plot(np.arange(len(rewards_one)), avg_10_reward1)\n",
    "plt.plot(np.arange(len(rewards_11)), avg_10_reward11)\n",
    "plt.legend(['Old Options', 'New Options'])\n",
    "plt.savefig('./intraop/old_v_new_rewards.jpg', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra-Option Q-Learning with Alternate Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def test_hyperparameters():\n",
    "    with open('./logs/smdp_log2.txt', 'w') as f:\n",
    "        sys.stdout = f \n",
    "        alphas = [0.5, 0.1, 0.05, 0.01]\n",
    "        gammas = [0.90]\n",
    "        epsilons = [0.1]\n",
    "\n",
    "        best_reward = -float('inf')\n",
    "        best_hyperparams = {'alpha': None, 'gamma': None, 'epsilon': None} \n",
    "\n",
    "        config = 1\n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for epsilon in epsilons:\n",
    "                    print(\"Testing Configuration:\", config)\n",
    "                    print('Hyperparameters: [alpha = {}, gamma = {}, epsilon = {}]'.format(alpha, gamma, epsilon))\n",
    "                    agent = SMDPTrainer(alpha=alpha, epsilon=epsilon, gamma=gamma, opt_fns=options_funcs_new, gen_avl_options=generate_available_options)\n",
    "                    rewards, q_values, update_freq = agent.trainer(verbose=False)\n",
    "                    avg_reward = np.mean(rewards)\n",
    "                    print('Average Reward:', avg_reward)\n",
    "                    print('***************************************************************************\\n')\n",
    "\n",
    "                    if avg_reward > best_reward:\n",
    "                        best_reward = avg_reward\n",
    "                        best_hyperparams['alpha'] = alpha\n",
    "                        best_hyperparams['gamma'] = gamma\n",
    "                        best_hyperparams['epsilon'] = epsilon\n",
    "\n",
    "                    config += 1\n",
    "\n",
    "        print('\\nBest Reward:', best_reward)\n",
    "        print('Best Hyperparameters:', best_hyperparams)\n",
    "\n",
    "# Call the function to execute the hyperparameter testing\n",
    "test_hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "reward_list = []\n",
    "\n",
    "# Training the SMDP agent for 5 episodes\n",
    "for i in range(5):\n",
    "    agent = SMDPTrainer(alpha=alpha, epsilon=epsilon, gamma=gamma, opt_fns=options_funcs_new, gen_avl_options=generate_available_options)\n",
    "    rewards, _, _ = agent.trainer(verbose=True)\n",
    "    reward_list.append(rewards)\n",
    "\n",
    "# Calculating average rewards and the moving average\n",
    "reward_array = np.array(reward_list)\n",
    "eps_rewards = np.mean(reward_array, axis=0)\n",
    "avg100_reward = np.array([np.mean(eps_rewards[max(0, i - 100):i]) for i in range(1, len(eps_rewards) + 1)])\n",
    "\n",
    "# Plotting the rewards vs episodes\n",
    "plt.style.use(\"darkgrid\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.title('Rewards vs Episodes: Avg Reward: %.3f' % np.mean(eps_rewards))\n",
    "plt.plot(np.arange(3000), eps_rewards, 'b')\n",
    "plt.plot(np.arange(3000), avg100_reward, 'r', linewidth=1.5)\n",
    "plt.savefig('./smdp/rewards_new.jpg', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_01 = SMDPTrainer(alpha_value=0.5, epsilon_value=0.1, gamma_rate=0.9, opt_fns=options_funcs_new, gen_avl_options=generate_available_options)\n",
    "\n",
    "# Train the agent and retrieve rewards, Q-values, and update frequencies\n",
    "rewards_01, q_values_01, update_frequencies_01 = agent_01.train(verbose=True)\n",
    "\n",
    "# Plot the reward curve and save the plot\n",
    "agent_01.plot_rewards(save=True)\n",
    "\n",
    "# Plot the update frequency and save the plot\n",
    "agent_01.plot_update_frequency(save=True)\n",
    "\n",
    "# Plot the Q-values and save the plot\n",
    "agent_01.plot_q_values(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plotting style\n",
    "plt.style.use(\"darkgrid\")\n",
    "\n",
    "# Calculate moving averages for old and new options\n",
    "avg10_reward_old = np.array([np.mean(rewards_zero[max(0, i - 10):i]) for i in range(1, len(rewards_zero) + 1)])\n",
    "avg10_reward_new = np.array([np.mean(rewards_01[max(0, i - 10):i]) for i in range(1, len(rewards_01) + 1)])\n",
    "\n",
    "# Plotting the rewards vs episodes for old and new options\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Reward')\n",
    "plt.title('Rewards for IntraOp - Old vs New Options')\n",
    "plt.plot(np.arange(len(rewards_zero)), avg10_reward_old)\n",
    "plt.plot(np.arange(len(rewards_01)), avg10_reward_new)\n",
    "plt.legend(['Old Options', 'New Options'])\n",
    "plt.savefig('./smdp/old_v_new_rewards.jpg', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
